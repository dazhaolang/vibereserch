"""
å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†APIå’Œä»»åŠ¡é›†æˆ
"""

import asyncio
from datetime import datetime
from typing import List, Dict, Optional, Any
from fastapi import HTTPException
from loguru import logger
from sqlalchemy.orm import Session

from app.core.database import SessionLocal
from app.models.task import Task
from app.models.project import Project
from app.services.massive_literature_processor import (
    MassiveLiteratureProcessor,
    start_massive_literature_processing
)
from app.services.stream_progress_service import StreamProgressService
from app.tasks.literature_tasks import safe_broadcast_update


async def start_massive_processing_task(
    task_id: int,
    project_id: int,
    processing_config: Dict[str, Any] = None
):
    """
    å¯åŠ¨å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†ä»»åŠ¡

    Args:
        task_id: ä»»åŠ¡ID
        project_id: é¡¹ç›®ID
        processing_config: å¤„ç†é…ç½®å‚æ•°
    """
    db = SessionLocal()
    progress_service = StreamProgressService()

    try:
        # è·å–ä»»åŠ¡å’Œé¡¹ç›®ä¿¡æ¯
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

        project = db.query(Project).filter(Project.id == project_id).first()
        if not project:
            raise ValueError(f"é¡¹ç›®ä¸å­˜åœ¨: {project_id}")

        # è§£æå¤„ç†é…ç½®
        config = processing_config or {}
        batch_size = config.get("batch_size", 20)
        max_concurrent = config.get("max_concurrent", 10)
        max_retries = config.get("max_retries", 3)
        memory_limit = config.get("memory_limit", 8.0)
        resume_from_checkpoint = config.get("resume_from_checkpoint", True)

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        task.status = "running"
        task.started_at = datetime.utcnow()
        task.progress_percentage = 0
        task.current_step = "ğŸš€ åˆå§‹åŒ–å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†å¼•æ“..."
        task.result = {
            "processing_config": config,
            "start_time": datetime.utcnow().isoformat()
        }
        db.commit()

        # å‘é€ä»»åŠ¡å¼€å§‹é€šçŸ¥
        await safe_broadcast_update(progress_service, task_id, {
            "type": "massive_processing_started",
            "task_id": task_id,
            "project_id": project_id,
            "progress": 0,
            "current_step": "ğŸš€ åˆå§‹åŒ–å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†å¼•æ“...",
            "config": {
                "batch_size": batch_size,
                "max_concurrent": max_concurrent,
                "memory_limit": f"{memory_limit}GB",
                "resume_enabled": resume_from_checkpoint
            }
        })

        logger.info(f"å¼€å§‹å¤§è§„æ¨¡å¤„ç†ä»»åŠ¡ - Task ID: {task_id}, Project ID: {project_id}")

        # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
        processor = MassiveLiteratureProcessor(
            batch_size=batch_size,
            max_concurrent=max_concurrent,
            max_retries=max_retries,
            memory_limit=memory_limit
        )

        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
        async def progress_callback(step: str, progress: int, details: dict = None):
            try:
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                task_update = db.query(Task).filter(Task.id == task_id).first()
                if task_update:
                    task_update.progress_percentage = progress
                    task_update.current_step = step

                    if details:
                        current_result = task_update.result or {}
                        current_result.update(details)
                        task_update.result = current_result

                    db.commit()

                # å‘é€WebSocketæ›´æ–°
                await safe_broadcast_update(progress_service, task_id, {
                    "type": "massive_processing_progress",
                    "task_id": task_id,
                    "project_id": project_id,
                    "progress": progress,
                    "current_step": step,
                    "details": details or {},
                    "timestamp": datetime.utcnow().isoformat()
                })

                logger.info(f"å¤§è§„æ¨¡å¤„ç†è¿›åº¦ - Task {task_id}: {step} ({progress}%)")

            except Exception as e:
                logger.error(f"è¿›åº¦æ›´æ–°å¤±è´¥: {e}")

        # æ‰§è¡Œå¤§è§„æ¨¡å¤„ç†
        processing_results = await processor.process_project_literature(
            project_id=project_id,
            task_id=task_id,
            resume_from_checkpoint=resume_from_checkpoint,
            progress_callback=progress_callback
        )

        # å¤„ç†å®Œæˆï¼Œæ›´æ–°ä»»åŠ¡çŠ¶æ€
        task = db.query(Task).filter(Task.id == task_id).first()
        if task:
            task.status = "completed"
            task.progress_percentage = 100
            task.current_step = "âœ… å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†å®Œæˆ"
            task.completed_at = datetime.utcnow()

            # è®¡ç®—å®é™…è€—æ—¶
            if task.started_at:
                task.actual_duration = int((task.completed_at - task.started_at).total_seconds())

            # ä¿å­˜è¯¦ç»†ç»“æœ
            task.result = {
                **task.result,
                "completion_time": datetime.utcnow().isoformat(),
                "processing_results": processing_results,
                "performance_summary": {
                    "total_literature": processing_results.get("total_literature", 0),
                    "successful_count": processing_results.get("successful", 0),
                    "failed_count": processing_results.get("failed", 0),
                    "success_rate": f"{(processing_results.get('successful', 0) / max(processing_results.get('total_literature', 1), 1) * 100):.1f}%",
                    "processing_time": f"{processing_results.get('processing_time', 0):.1f}ç§’",
                    "throughput": f"{processing_results.get('throughput', 0):.2f} ç¯‡/ç§’",
                    "memory_peak": f"{processing_results.get('memory_peak', 0):.2f}GB",
                    "tokens_used": processing_results.get("tokens_used", 0)
                }
            }
            db.commit()

        # å‘é€å®Œæˆé€šçŸ¥
        await safe_broadcast_update(progress_service, task_id, {
            "type": "massive_processing_completed",
            "task_id": task_id,
            "project_id": project_id,
            "progress": 100,
            "current_step": "âœ… å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†å®Œæˆ",
            "results": processing_results,
            "performance_summary": task.result.get("performance_summary", {}),
            "completion_time": datetime.utcnow().isoformat()
        })

        logger.info(f"å¤§è§„æ¨¡å¤„ç†ä»»åŠ¡å®Œæˆ - Task ID: {task_id}, æˆåŠŸ: {processing_results.get('successful', 0)}, å¤±è´¥: {processing_results.get('failed', 0)}")

        return processing_results

    except Exception as e:
        logger.error(f"å¤§è§„æ¨¡å¤„ç†ä»»åŠ¡å¤±è´¥ - Task ID: {task_id}: {e}")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        try:
            task = db.query(Task).filter(Task.id == task_id).first()
            if task:
                task.status = "failed"
                task.error_message = str(e)
                task.completed_at = datetime.utcnow()

                if task.started_at:
                    task.actual_duration = int((task.completed_at - task.started_at).total_seconds())

                current_result = task.result or {}
                current_result.update({
                    "error": str(e),
                    "error_time": datetime.utcnow().isoformat()
                })
                task.result = current_result

                db.commit()

            # å‘é€å¤±è´¥é€šçŸ¥
            await safe_broadcast_update(progress_service, task_id, {
                "type": "massive_processing_failed",
                "task_id": task_id,
                "project_id": project_id,
                "error": str(e),
                "error_time": datetime.utcnow().isoformat()
            })

        except Exception as commit_error:
            logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {commit_error}")

        raise

    finally:
        db.close()


async def get_massive_processing_status(session_id: str) -> Dict[str, Any]:
    """
    è·å–å¤§è§„æ¨¡å¤„ç†çŠ¶æ€

    Args:
        session_id: ä¼šè¯ID

    Returns:
        å¤„ç†çŠ¶æ€ä¿¡æ¯
    """
    try:
        # è¿™é‡Œå¯ä»¥é€šè¿‡session_idæŸ¥è¯¢å¤„ç†çŠ¶æ€
        # å®é™…å®ç°ä¸­å¯èƒ½éœ€è¦ç»´æŠ¤ä¸€ä¸ªå…¨å±€çš„å¤„ç†å™¨å®ä¾‹ç®¡ç†å™¨

        return {
            "success": True,
            "message": "çŠ¶æ€æŸ¥è¯¢åŠŸèƒ½éœ€è¦å¤„ç†å™¨å®ä¾‹ç®¡ç†å™¨æ”¯æŒ",
            "session_id": session_id
        }

    except Exception as e:
        logger.error(f"è·å–å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


async def stop_massive_processing(session_id: str, save_checkpoint: bool = True) -> Dict[str, Any]:
    """
    åœæ­¢å¤§è§„æ¨¡å¤„ç†

    Args:
        session_id: ä¼šè¯ID
        save_checkpoint: æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹

    Returns:
        åœæ­¢æ“ä½œç»“æœ
    """
    try:
        # è¿™é‡Œéœ€è¦å®ç°å¤„ç†å™¨å®ä¾‹ç®¡ç†å’Œåœæ­¢é€»è¾‘
        logger.info(f"è¯·æ±‚åœæ­¢å¤§è§„æ¨¡å¤„ç† - Session ID: {session_id}")

        return {
            "success": True,
            "message": "åœæ­¢è¯·æ±‚å·²å‘é€",
            "session_id": session_id,
            "checkpoint_saved": save_checkpoint
        }

    except Exception as e:
        logger.error(f"åœæ­¢å¤„ç†å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


# é«˜çº§åŠŸèƒ½ï¼šæ™ºèƒ½æ‰¹é‡å¤„ç†é…ç½®ä¼˜åŒ–
class ProcessingConfigOptimizer:
    """å¤„ç†é…ç½®ä¼˜åŒ–å™¨"""

    @staticmethod
    async def optimize_config_for_project(project_id: int, literature_count: int) -> Dict[str, Any]:
        """
        ä¸ºé¡¹ç›®ä¼˜åŒ–å¤„ç†é…ç½®

        Args:
            project_id: é¡¹ç›®ID
            literature_count: æ–‡çŒ®æ•°é‡

        Returns:
            ä¼˜åŒ–çš„é…ç½®å‚æ•°
        """
        try:
            # è·å–ç³»ç»Ÿèµ„æºä¿¡æ¯
            import psutil
            memory_total = psutil.virtual_memory().total / (1024**3)  # GB
            cpu_count = psutil.cpu_count()

            # åŸºäºæ–‡çŒ®æ•°é‡å’Œç³»ç»Ÿèµ„æºä¼˜åŒ–é…ç½®
            if literature_count <= 50:
                # å°è§„æ¨¡å¤„ç†
                config = {
                    "batch_size": 10,
                    "max_concurrent": min(5, cpu_count),
                    "memory_limit": min(2.0, memory_total * 0.3),
                    "processing_mode": "standard"
                }
            elif literature_count <= 200:
                # ä¸­ç­‰è§„æ¨¡å¤„ç†
                config = {
                    "batch_size": 20,
                    "max_concurrent": min(10, cpu_count),
                    "memory_limit": min(4.0, memory_total * 0.5),
                    "processing_mode": "optimized"
                }
            else:
                # å¤§è§„æ¨¡å¤„ç†
                config = {
                    "batch_size": 30,
                    "max_concurrent": min(20, cpu_count),
                    "memory_limit": min(8.0, memory_total * 0.7),
                    "processing_mode": "massive"
                }

            # æ·»åŠ é€šç”¨ä¼˜åŒ–å‚æ•°
            config.update({
                "max_retries": 3,
                "checkpoint_interval": 300,  # 5åˆ†é’Ÿ
                "resume_from_checkpoint": True,
                "enable_performance_monitoring": True,
                "auto_gc_interval": 50  # æ¯50ä¸ªé¡¹ç›®å¼ºåˆ¶åƒåœ¾å›æ”¶
            })

            # é¢„ä¼°å¤„ç†æ—¶é—´
            estimated_time_per_item = 3  # ç§’/ç¯‡ (åŸºäºå†å²æ•°æ®)
            parallel_factor = config["max_concurrent"] * 0.7  # è€ƒè™‘å¹¶è¡Œæ•ˆç‡

            estimated_total_time = (literature_count * estimated_time_per_item) / parallel_factor

            config["estimated_processing_time"] = {
                "total_seconds": int(estimated_total_time),
                "formatted": f"{int(estimated_total_time // 60)}åˆ†{int(estimated_total_time % 60)}ç§’"
            }

            logger.info(f"ä¸ºé¡¹ç›® {project_id} ä¼˜åŒ–é…ç½® - æ–‡çŒ®æ•°é‡: {literature_count}, é¢„è®¡è€—æ—¶: {config['estimated_processing_time']['formatted']}")

            return {
                "success": True,
                "config": config,
                "system_info": {
                    "total_memory": f"{memory_total:.1f}GB",
                    "cpu_cores": cpu_count,
                    "literature_count": literature_count
                }
            }

        except Exception as e:
            logger.error(f"é…ç½®ä¼˜åŒ–å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "default_config": {
                    "batch_size": 20,
                    "max_concurrent": 10,
                    "memory_limit": 4.0,
                    "max_retries": 3
                }
            }


# æ€§èƒ½ç›‘æ§å’ŒæŠ¥å‘Š
class MassiveProcessingReporter:
    """å¤§è§„æ¨¡å¤„ç†æŠ¥å‘Šç”Ÿæˆå™¨"""

    @staticmethod
    async def generate_processing_report(
        task_id: int,
        include_detailed_metrics: bool = True
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤„ç†æŠ¥å‘Š

        Args:
            task_id: ä»»åŠ¡ID
            include_detailed_metrics: æ˜¯å¦åŒ…å«è¯¦ç»†æŒ‡æ ‡

        Returns:
            å¤„ç†æŠ¥å‘Š
        """
        db = SessionLocal()

        try:
            task = db.query(Task).filter(Task.id == task_id).first()
            if not task:
                raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

            task_result = task.result or {}
            processing_results = task_result.get("processing_results", {})

            # åŸºç¡€æŠ¥å‘Š
            report = {
                "task_info": {
                    "task_id": task_id,
                    "project_id": task.project_id,
                    "status": task.status,
                    "start_time": task.started_at.isoformat() if task.started_at else None,
                    "end_time": task.completed_at.isoformat() if task.completed_at else None,
                    "duration": task.actual_duration or 0
                },
                "summary": task_result.get("performance_summary", {}),
                "processing_overview": {
                    "total_literature": processing_results.get("total_literature", 0),
                    "successful": processing_results.get("successful", 0),
                    "failed": processing_results.get("failed", 0),
                    "skipped": processing_results.get("skipped", 0),
                    "success_rate": processing_results.get("success_rate", 0)
                }
            }

            if include_detailed_metrics and "statistics" in processing_results:
                stats = processing_results["statistics"]

                report["detailed_metrics"] = {
                    "performance": {
                        "throughput_per_minute": stats.get("throughput_per_minute", 0),
                        "average_processing_time": stats.get("average_processing_time", 0),
                        "total_segments_generated": stats.get("total_segments_generated", 0),
                        "total_tokens_used": stats.get("total_tokens_used", 0)
                    },
                    "resource_usage": {
                        "memory_peak": stats.get("performance_metrics", {}).get("peak_memory", 0),
                        "cpu_peak": stats.get("performance_metrics", {}).get("peak_cpu", 0),
                        "memory_efficiency": stats.get("memory_efficiency", {})
                    },
                    "error_analysis": stats.get("error_analysis", {}),
                    "batch_performance": processing_results.get("batch_metrics", [])
                }

            # ç”Ÿæˆå»ºè®®
            report["recommendations"] = MassiveProcessingReporter._generate_recommendations(
                processing_results
            )

            return {
                "success": True,
                "report": report,
                "generated_at": datetime.utcnow().isoformat()
            }

        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤„ç†æŠ¥å‘Šå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

        finally:
            db.close()

    @staticmethod
    def _generate_recommendations(processing_results: Dict[str, Any]) -> List[Dict[str, str]]:
        """åŸºäºå¤„ç†ç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        success_rate = processing_results.get("success_rate", 0)
        throughput = processing_results.get("throughput", 0)
        memory_peak = processing_results.get("memory_peak", 0)

        # æˆåŠŸç‡å»ºè®®
        if success_rate < 0.8:
            recommendations.append({
                "category": "è´¨é‡",
                "priority": "é«˜",
                "suggestion": "æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥PDFå¤„ç†é…ç½®å’Œæ–‡çŒ®è´¨é‡ï¼Œè€ƒè™‘å¢åŠ é‡è¯•æ¬¡æ•°"
            })
        elif success_rate > 0.95:
            recommendations.append({
                "category": "è´¨é‡",
                "priority": "ä¿¡æ¯",
                "suggestion": "å¤„ç†è´¨é‡ä¼˜ç§€ï¼Œå½“å‰é…ç½®é€‚åˆæ­¤ç±»æ–‡çŒ®"
            })

        # æ€§èƒ½å»ºè®®
        if throughput < 1.0:  # æ¯ç§’å°‘äº1ç¯‡
            recommendations.append({
                "category": "æ€§èƒ½",
                "priority": "ä¸­",
                "suggestion": "å¤„ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ å¹¶å‘æ•°æˆ–ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°"
            })
        elif throughput > 3.0:  # æ¯ç§’è¶…è¿‡3ç¯‡
            recommendations.append({
                "category": "æ€§èƒ½",
                "priority": "ä¿¡æ¯",
                "suggestion": "å¤„ç†é€Ÿåº¦ä¼˜ç§€ï¼Œç³»ç»Ÿæ€§èƒ½å¾—åˆ°å……åˆ†åˆ©ç”¨"
            })

        # å†…å­˜å»ºè®®
        if memory_peak > 6.0:  # å†…å­˜ä½¿ç”¨è¶…è¿‡6GB
            recommendations.append({
                "category": "èµ„æº",
                "priority": "ä¸­",
                "suggestion": "å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®å‡å°‘æ‰¹å¤„ç†å¤§å°æˆ–å¹¶å‘æ•°ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨"
            })

        # é€šç”¨å»ºè®®
        recommendations.append({
            "category": "ç»´æŠ¤",
            "priority": "ä½",
            "suggestion": "å®šæœŸæ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶å’Œä¸´æ—¶æ•°æ®ä»¥ç»´æŠ¤ç³»ç»Ÿæ€§èƒ½"
        })

        return recommendations


# ä¸ç°æœ‰ç³»ç»Ÿçš„é›†æˆæ¥å£
async def integrate_with_existing_literature_tasks(
    keywords: List[str],
    project_id: int,
    max_count: int = 200,
    enable_massive_processing: bool = True,
    processing_config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    é›†æˆç°æœ‰æ–‡çŒ®ä»»åŠ¡ç³»ç»Ÿçš„å¤§è§„æ¨¡å¤„ç†æ¥å£

    Args:
        keywords: æœç´¢å…³é”®è¯
        project_id: é¡¹ç›®ID
        max_count: æœ€å¤§æ–‡çŒ®æ•°é‡
        enable_massive_processing: æ˜¯å¦å¯ç”¨å¤§è§„æ¨¡å¤„ç†
        processing_config: å¤„ç†é…ç½®

    Returns:
        é›†æˆå¤„ç†ç»“æœ
    """
    try:
        logger.info(f"å¯åŠ¨é›†æˆæ–‡çŒ®å¤„ç† - é¡¹ç›®: {project_id}, å…³é”®è¯: {keywords}, æ•°é‡: {max_count}")

        # ç¬¬ä¸€é˜¶æ®µï¼šæ–‡çŒ®æœç´¢å’Œé‡‡é›†
        logger.info("é˜¶æ®µ1: æ–‡çŒ®æœç´¢å’Œé‡‡é›†")
        # è¿™é‡Œå¯ä»¥å¤ç”¨ç°æœ‰çš„literature_tasksä¸­çš„æœç´¢é€»è¾‘

        # ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½æ‰¹é‡å¤„ç†é…ç½®ä¼˜åŒ–
        logger.info("é˜¶æ®µ2: å¤„ç†é…ç½®ä¼˜åŒ–")
        optimizer = ProcessingConfigOptimizer()
        config_result = await optimizer.optimize_config_for_project(project_id, max_count)

        if not config_result["success"]:
            logger.warning(f"é…ç½®ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {config_result.get('error')}")
            optimized_config = config_result["default_config"]
        else:
            optimized_config = config_result["config"]

        # åˆå¹¶ç”¨æˆ·æä¾›çš„é…ç½®
        if processing_config:
            optimized_config.update(processing_config)

        # ç¬¬ä¸‰é˜¶æ®µï¼šå¤§è§„æ¨¡å¤„ç†
        if enable_massive_processing and max_count >= 50:
            logger.info("é˜¶æ®µ3: å¯åŠ¨å¤§è§„æ¨¡æ‰¹é‡å¤„ç†")

            # åˆ›å»ºå¤„ç†ä»»åŠ¡
            db = SessionLocal()
            try:
                task = Task(
                    project_id=project_id,
                    task_type="massive_literature_processing",
                    description=f"å¤§è§„æ¨¡æ–‡çŒ®å¤„ç† - å…³é”®è¯: {', '.join(keywords)}",
                    status="pending",
                    estimated_duration=optimized_config.get("estimated_processing_time", {}).get("total_seconds", 3600)
                )
                db.add(task)
                db.commit()
                task_id = task.id

            finally:
                db.close()

            # å¯åŠ¨å¼‚æ­¥å¤„ç†ä»»åŠ¡
            asyncio.create_task(start_massive_processing_task(
                task_id=task_id,
                project_id=project_id,
                processing_config=optimized_config
            ))

            return {
                "success": True,
                "mode": "massive_processing",
                "task_id": task_id,
                "project_id": project_id,
                "estimated_time": optimized_config.get("estimated_processing_time", {}),
                "config_used": optimized_config,
                "message": f"å·²å¯åŠ¨å¤§è§„æ¨¡å¤„ç†ä»»åŠ¡ (ID: {task_id})ï¼Œé¢„è®¡å¤„ç† {max_count} ç¯‡æ–‡çŒ®"
            }
        else:
            logger.info("é˜¶æ®µ3: ä½¿ç”¨æ ‡å‡†å¤„ç†æ¨¡å¼")

            return {
                "success": True,
                "mode": "standard_processing",
                "message": f"æ–‡çŒ®æ•°é‡ ({max_count}) è¾ƒå°‘æˆ–æœªå¯ç”¨å¤§è§„æ¨¡å¤„ç†ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†æ¨¡å¼",
                "recommendation": "å¯¹äº50ç¯‡ä»¥ä¸Šæ–‡çŒ®å»ºè®®å¯ç”¨å¤§è§„æ¨¡å¤„ç†ä»¥è·å¾—æ›´å¥½æ€§èƒ½"
            }

    except Exception as e:
        logger.error(f"é›†æˆæ–‡çŒ®å¤„ç†å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }