"""
æ•°æ®å¯¼å…¥å¯¼å‡ºæœåŠ¡
"""

import asyncio
import json
import csv
import io
import zipfile
from typing import List, Dict, Optional, Any
from datetime import datetime
import pandas as pd
from pathlib import Path
from loguru import logger

from app.core.config import settings
from app.models.literature import Literature, LiteratureSegment
from app.models.project import Project
from app.models.experience import ExperienceBook, MainExperience

class DataExportService:
    """æ•°æ®å¯¼å‡ºæœåŠ¡"""
    
    def __init__(self):
        self.export_formats = ['json', 'csv', 'excel', 'pdf', 'markdown', 'bibtex', 'ris']
    
    async def export_project_data(
        self,
        project_id: int,
        export_format: str = 'json',
        include_options: Dict[str, bool] = None
    ) -> Dict[str, Any]:
        """
        å¯¼å‡ºé¡¹ç›®å®Œæ•´æ•°æ®
        
        Args:
            project_id: é¡¹ç›®ID
            export_format: å¯¼å‡ºæ ¼å¼
            include_options: åŒ…å«é€‰é¡¹
            
        Returns:
            å¯¼å‡ºç»“æœ
        """
        try:
            if include_options is None:
                include_options = {
                    'literature': True,
                    'segments': True,
                    'experience_books': True,
                    'main_experience': True,
                    'comments': False,
                    'activity_logs': False
                }
            
            # æ”¶é›†é¡¹ç›®æ•°æ®
            project_data = await self._collect_project_data(project_id, include_options)
            
            # æ ¹æ®æ ¼å¼å¯¼å‡º
            if export_format == 'json':
                return await self._export_to_json(project_data)
            elif export_format == 'csv':
                return await self._export_to_csv(project_data)
            elif export_format == 'excel':
                return await self._export_to_excel(project_data)
            elif export_format == 'pdf':
                return await self._export_to_pdf(project_data)
            elif export_format == 'markdown':
                return await self._export_to_markdown(project_data)
            elif export_format == 'bibtex':
                return await self._export_to_bibtex(project_data)
            elif export_format == 'ris':
                return await self._export_to_ris(project_data)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {export_format}")
                
        except Exception as e:
            logger.error(f"é¡¹ç›®æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _collect_project_data(self, project_id: int, include_options: Dict[str, bool]) -> Dict:
        """æ”¶é›†é¡¹ç›®æ•°æ®"""
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æŸ¥è¯¢çœŸå®æ•°æ®
        # ä¸ºäº†æµ‹è¯•ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        
        project_data = {
            "project": {
                "id": project_id,
                "name": f"æµ‹è¯•é¡¹ç›® {project_id}",
                "description": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é¡¹ç›®çš„æè¿°",
                "research_direction": "æ°®åŒ–ç¡¼çº³ç±³ææ–™ç ”ç©¶",
                "keywords": ["æ°®åŒ–ç¡¼", "çº³ç±³ææ–™", "åˆ¶å¤‡å·¥è‰º"],
                "created_at": datetime.now().isoformat()
            }
        }
        
        if include_options.get('literature', False):
            project_data["literature"] = [
                {
                    "id": 1,
                    "title": "Advanced Synthesis of Boron Nitride Nanosheets",
                    "authors": ["John Smith", "Jane Doe"],
                    "abstract": "This study presents novel synthesis methods...",
                    "journal": "Nature Materials",
                    "publication_year": 2023,
                    "citation_count": 156,
                    "doi": "10.1038/s41563-023-1234-5"
                },
                {
                    "id": 2,
                    "title": "Thermal Properties of BN Nanocomposites",
                    "authors": ["Wei Zhang", "Ming Li"],
                    "abstract": "Investigation of thermal conductivity...",
                    "journal": "Advanced Materials", 
                    "publication_year": 2023,
                    "citation_count": 89,
                    "doi": "10.1002/adma.202301234"
                }
            ]
        
        if include_options.get('experience_books', False):
            project_data["experience_books"] = [
                {
                    "id": 1,
                    "title": "æ°®åŒ–ç¡¼åˆ¶å¤‡ç»éªŒä¹¦ - ç¬¬5è½®",
                    "research_question": "å¦‚ä½•ä¼˜åŒ–çƒç£¨æ³•åˆ¶å¤‡æ°®åŒ–ç¡¼çº³ç±³ç‰‡ï¼Ÿ",
                    "iteration_round": 5,
                    "content": "åŸºäºå¤šç¯‡æ–‡çŒ®çš„åˆ†æï¼Œçƒç£¨æ³•åˆ¶å¤‡æ°®åŒ–ç¡¼çº³ç±³ç‰‡çš„æœ€ä½³å·¥è‰ºå‚æ•°ä¸º...",
                    "information_gain": 0.08,
                    "is_final": True
                }
            ]
        
        if include_options.get('main_experience', False):
            project_data["main_experience"] = {
                "id": 1,
                "title": "æ°®åŒ–ç¡¼çº³ç±³ææ–™ä¸»ç»éªŒåº“",
                "research_domain": "çº³ç±³ææ–™",
                "content": "æ°®åŒ–ç¡¼çº³ç±³ææ–™çš„åˆ¶å¤‡ã€è¡¨å¾å’Œåº”ç”¨ç»¼åˆç»éªŒ...",
                "coverage_scope": ["çƒç£¨æ³•", "è¶…å£°æ³•", "CVDæ³•"],
                "source_literature_count": 150
            }
        
        return project_data
    
    async def _export_to_json(self, data: Dict) -> Dict[str, Any]:
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        try:
            json_content = json.dumps(data, ensure_ascii=False, indent=2)
            
            return {
                "success": True,
                "format": "json",
                "content": json_content,
                "filename": f"project_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                "size": len(json_content.encode('utf-8'))
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _export_to_csv(self, data: Dict) -> Dict[str, Any]:
        """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
        try:
            csv_files = {}
            
            # å¯¼å‡ºæ–‡çŒ®æ•°æ®
            if 'literature' in data:
                literature_csv = io.StringIO()
                writer = csv.writer(literature_csv)
                
                # å†™å…¥å¤´éƒ¨
                headers = ['ID', 'Title', 'Authors', 'Journal', 'Year', 'Citations', 'DOI', 'Abstract']
                writer.writerow(headers)
                
                # å†™å…¥æ•°æ®
                for lit in data['literature']:
                    row = [
                        lit.get('id', ''),
                        lit.get('title', ''),
                        '; '.join(lit.get('authors', [])),
                        lit.get('journal', ''),
                        lit.get('publication_year', ''),
                        lit.get('citation_count', 0),
                        lit.get('doi', ''),
                        lit.get('abstract', '')[:500] + '...' if len(lit.get('abstract', '')) > 500 else lit.get('abstract', '')
                    ]
                    writer.writerow(row)
                
                csv_files['literature.csv'] = literature_csv.getvalue()
            
            # åˆ›å»ºZIPæ–‡ä»¶åŒ…å«æ‰€æœ‰CSV
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                for filename, content in csv_files.items():
                    zip_file.writestr(filename, content)
                
                # æ·»åŠ é¡¹ç›®ä¿¡æ¯
                project_info = json.dumps(data.get('project', {}), ensure_ascii=False, indent=2)
                zip_file.writestr('project_info.json', project_info)
            
            return {
                "success": True,
                "format": "csv",
                "content": zip_buffer.getvalue(),
                "filename": f"project_csv_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                "size": len(zip_buffer.getvalue())
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _export_to_markdown(self, data: Dict) -> Dict[str, Any]:
        """å¯¼å‡ºä¸ºMarkdownæ ¼å¼"""
        try:
            md_content = []
            
            # é¡¹ç›®ä¿¡æ¯
            project = data.get('project', {})
            md_content.append(f"# {project.get('name', 'é¡¹ç›®å¯¼å‡º')}")
            md_content.append(f"\n**é¡¹ç›®æè¿°**: {project.get('description', '')}")
            md_content.append(f"**ç ”ç©¶æ–¹å‘**: {project.get('research_direction', '')}")
            md_content.append(f"**å…³é”®è¯**: {', '.join(project.get('keywords', []))}")
            md_content.append(f"**å¯¼å‡ºæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
            md_content.append("\n---\n")
            
            # æ–‡çŒ®åˆ—è¡¨
            if 'literature' in data:
                md_content.append("## ğŸ“š æ–‡çŒ®åº“\n")
                
                for i, lit in enumerate(data['literature'], 1):
                    md_content.append(f"### {i}. {lit.get('title', '')}")
                    md_content.append(f"**ä½œè€…**: {', '.join(lit.get('authors', []))}")
                    md_content.append(f"**æœŸåˆŠ**: {lit.get('journal', '')}")
                    md_content.append(f"**å¹´ä»½**: {lit.get('publication_year', '')}")
                    md_content.append(f"**å¼•ç”¨æ•°**: {lit.get('citation_count', 0)}")
                    
                    if lit.get('doi'):
                        md_content.append(f"**DOI**: {lit['doi']}")
                    
                    if lit.get('abstract'):
                        md_content.append(f"\n**æ‘˜è¦**: {lit['abstract']}")
                    
                    md_content.append("\n---\n")
            
            # ä¸»ç»éªŒ
            if 'main_experience' in data:
                main_exp = data['main_experience']
                md_content.append("## ğŸ§  ä¸»ç»éªŒåº“\n")
                md_content.append(f"**æ ‡é¢˜**: {main_exp.get('title', '')}")
                md_content.append(f"**ç ”ç©¶é¢†åŸŸ**: {main_exp.get('research_domain', '')}")
                md_content.append(f"**è¦†ç›–èŒƒå›´**: {', '.join(main_exp.get('coverage_scope', []))}")
                md_content.append(f"**åŸºäºæ–‡çŒ®æ•°**: {main_exp.get('source_literature_count', 0)}")
                md_content.append(f"\n{main_exp.get('content', '')}")
                md_content.append("\n---\n")
            
            # ç»éªŒä¹¦
            if 'experience_books' in data:
                md_content.append("## ğŸ“– ç»éªŒä¹¦\n")
                
                for book in data['experience_books']:
                    md_content.append(f"### {book.get('title', '')}")
                    md_content.append(f"**ç ”ç©¶é—®é¢˜**: {book.get('research_question', '')}")
                    md_content.append(f"**è¿­ä»£è½®æ¬¡**: ç¬¬{book.get('iteration_round', 0)}è½®")
                    md_content.append(f"**ä¿¡æ¯å¢ç›Š**: {book.get('information_gain', 0):.2%}")
                    md_content.append(f"\n{book.get('content', '')}")
                    md_content.append("\n---\n")
            
            markdown_text = '\n'.join(md_content)
            
            return {
                "success": True,
                "format": "markdown",
                "content": markdown_text,
                "filename": f"project_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                "size": len(markdown_text.encode('utf-8'))
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _export_to_bibtex(self, data: Dict) -> Dict[str, Any]:
        """å¯¼å‡ºä¸ºBibTeXæ ¼å¼"""
        try:
            bibtex_entries = []
            
            if 'literature' in data:
                for lit in data['literature']:
                    # ç”Ÿæˆå¼•ç”¨é”®
                    first_author = lit.get('authors', ['Unknown'])[0].split()[-1] if lit.get('authors') else 'Unknown'
                    year = lit.get('publication_year', datetime.now().year)
                    cite_key = f"{first_author.lower()}{year}"
                    
                    # æ„å»ºBibTeXæ¡ç›®
                    entry = f"@article{{{cite_key},\n"
                    entry += f"  title={{{lit.get('title', '')}}},\n"
                    
                    if lit.get('authors'):
                        authors = ' and '.join(lit['authors'])
                        entry += f"  author={{{authors}}},\n"
                    
                    if lit.get('journal'):
                        entry += f"  journal={{{lit['journal']}}},\n"
                    
                    if lit.get('publication_year'):
                        entry += f"  year={{{lit['publication_year']}}},\n"
                    
                    if lit.get('doi'):
                        entry += f"  doi={{{lit['doi']}}},\n"
                    
                    if lit.get('abstract'):
                        entry += f"  abstract={{{lit['abstract']}}},\n"
                    
                    entry = entry.rstrip(',\n') + '\n}'
                    bibtex_entries.append(entry)
            
            bibtex_content = '\n\n'.join(bibtex_entries)
            
            return {
                "success": True,
                "format": "bibtex",
                "content": bibtex_content,
                "filename": f"literature_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.bib",
                "size": len(bibtex_content.encode('utf-8'))
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def export_literature_list(
        self,
        literature_ids: List[int],
        export_format: str = 'json'
    ) -> Dict[str, Any]:
        """å¯¼å‡ºæ–‡çŒ®åˆ—è¡¨"""
        try:
            # æ¨¡æ‹Ÿä»æ•°æ®åº“è·å–æ–‡çŒ®æ•°æ®
            literature_data = []
            for lit_id in literature_ids:
                literature_data.append({
                    "id": lit_id,
                    "title": f"Literature {lit_id}",
                    "authors": [f"Author {lit_id}A", f"Author {lit_id}B"],
                    "journal": f"Journal {lit_id % 3 + 1}",
                    "publication_year": 2020 + (lit_id % 4),
                    "citation_count": lit_id * 10,
                    "quality_score": 50 + (lit_id % 5) * 10
                })
            
            if export_format == 'json':
                content = json.dumps(literature_data, ensure_ascii=False, indent=2)
            elif export_format == 'csv':
                output = io.StringIO()
                writer = csv.writer(output)
                
                # å†™å…¥å¤´éƒ¨
                writer.writerow(['ID', 'Title', 'Authors', 'Journal', 'Year', 'Citations', 'Quality Score'])
                
                # å†™å…¥æ•°æ®
                for lit in literature_data:
                    writer.writerow([
                        lit['id'],
                        lit['title'],
                        '; '.join(lit['authors']),
                        lit['journal'],
                        lit['publication_year'],
                        lit['citation_count'],
                        lit['quality_score']
                    ])
                
                content = output.getvalue()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {export_format}")
            
            return {
                "success": True,
                "format": export_format,
                "content": content,
                "filename": f"literature_list_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{export_format}",
                "count": len(literature_data)
            }
            
        except Exception as e:
            logger.error(f"æ–‡çŒ®åˆ—è¡¨å¯¼å‡ºå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

class DataImportService:
    """æ•°æ®å¯¼å…¥æœåŠ¡"""
    
    def __init__(self):
        self.supported_formats = ['json', 'csv', 'excel', 'ris', 'bibtex']
    
    async def import_literature_data(
        self,
        file_content: str,
        file_format: str,
        project_id: int,
        import_options: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        å¯¼å…¥æ–‡çŒ®æ•°æ®
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹
            file_format: æ–‡ä»¶æ ¼å¼
            project_id: ç›®æ ‡é¡¹ç›®ID
            import_options: å¯¼å…¥é€‰é¡¹
            
        Returns:
            å¯¼å…¥ç»“æœ
        """
        try:
            if import_options is None:
                import_options = {
                    'skip_duplicates': True,
                    'auto_process': True,
                    'quality_threshold': 30
                }
            
            # æ ¹æ®æ ¼å¼è§£ææ•°æ®
            if file_format == 'json':
                parsed_data = await self._parse_json_import(file_content)
            elif file_format == 'csv':
                parsed_data = await self._parse_csv_import(file_content)
            elif file_format == 'ris':
                parsed_data = await self._parse_ris_import(file_content)
            elif file_format == 'bibtex':
                parsed_data = await self._parse_bibtex_import(file_content)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å…¥æ ¼å¼: {file_format}")
            
            # æ•°æ®éªŒè¯å’Œæ¸…ç†
            validated_data = await self._validate_import_data(parsed_data, import_options)
            
            # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆæ¨¡æ‹Ÿï¼‰
            import_result = await self._save_imported_data(validated_data, project_id)
            
            return {
                "success": True,
                "imported_count": import_result["saved_count"],
                "skipped_count": import_result["skipped_count"],
                "error_count": import_result["error_count"],
                "total_processed": len(parsed_data),
                "details": import_result["details"]
            }
            
        except Exception as e:
            logger.error(f"æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "imported_count": 0
            }
    
    async def _parse_json_import(self, content: str) -> List[Dict]:
        """è§£æJSONå¯¼å…¥æ•°æ®"""
        try:
            data = json.loads(content)
            
            # æ”¯æŒå¤šç§JSONç»“æ„
            if isinstance(data, list):
                return data
            elif isinstance(data, dict):
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡çŒ®åˆ—è¡¨
                if 'literature' in data:
                    return data['literature']
                elif 'items' in data:
                    return data['items']
                else:
                    return [data]  # å•ä¸ªæ–‡çŒ®å¯¹è±¡
            else:
                raise ValueError("æ— æ•ˆçš„JSONæ ¼å¼")
                
        except json.JSONDecodeError as e:
            raise ValueError(f"JSONè§£æå¤±è´¥: {e}")
    
    async def _parse_csv_import(self, content: str) -> List[Dict]:
        """è§£æCSVå¯¼å…¥æ•°æ®"""
        try:
            # ä½¿ç”¨pandasè§£æCSV
            df = pd.read_csv(io.StringIO(content))
            
            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'title': ['title', 'Title', 'æ ‡é¢˜'],
                'authors': ['authors', 'Authors', 'Author', 'ä½œè€…'],
                'journal': ['journal', 'Journal', 'Publication', 'æœŸåˆŠ'],
                'year': ['year', 'Year', 'Publication Year', 'å¹´ä»½'],
                'doi': ['doi', 'DOI'],
                'abstract': ['abstract', 'Abstract', 'æ‘˜è¦']
            }
            
            # é‡å‘½ååˆ—
            for standard_name, possible_names in column_mapping.items():
                for col in df.columns:
                    if col in possible_names:
                        df = df.rename(columns={col: standard_name})
                        break
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            literature_list = []
            for _, row in df.iterrows():
                lit_data = {}
                
                for col in df.columns:
                    value = row[col]
                    if pd.notna(value):
                        if col == 'authors' and isinstance(value, str):
                            # åˆ†å‰²ä½œè€…å­—ç¬¦ä¸²
                            lit_data[col] = [author.strip() for author in value.split(';')]
                        else:
                            lit_data[col] = value
                
                if lit_data.get('title'):  # å¿…é¡»æœ‰æ ‡é¢˜
                    literature_list.append(lit_data)
            
            return literature_list
            
        except Exception as e:
            raise ValueError(f"CSVè§£æå¤±è´¥: {e}")
    
    async def _validate_import_data(self, data: List[Dict], options: Dict) -> List[Dict]:
        """éªŒè¯å’Œæ¸…ç†å¯¼å…¥æ•°æ®"""
        validated_data = []
        
        for item in data:
            # åŸºç¡€éªŒè¯
            if not item.get('title'):
                continue
            
            # è´¨é‡æ£€æŸ¥
            quality_score = self._calculate_import_quality_score(item)
            if quality_score < options.get('quality_threshold', 30):
                continue
            
            # æ ‡å‡†åŒ–æ•°æ®
            standardized_item = {
                'title': item.get('title', '').strip(),
                'authors': item.get('authors', []) if isinstance(item.get('authors'), list) else [item.get('authors', '')],
                'abstract': item.get('abstract', '').strip(),
                'journal': item.get('journal', '').strip(),
                'publication_year': self._parse_year(item.get('year') or item.get('publication_year')),
                'doi': item.get('doi', '').strip(),
                'source_url': item.get('url', '').strip(),
                'keywords': item.get('keywords', []) if isinstance(item.get('keywords'), list) else [],
                'quality_score': quality_score,
                'source_platform': 'import'
            }
            
            validated_data.append(standardized_item)
        
        return validated_data
    
    def _calculate_import_quality_score(self, item: Dict) -> float:
        """è®¡ç®—å¯¼å…¥æ–‡çŒ®çš„è´¨é‡è¯„åˆ†"""
        score = 0.0
        
        # æ ‡é¢˜ (20åˆ†)
        if item.get('title'):
            score += 20
        
        # ä½œè€… (20åˆ†)
        authors = item.get('authors', [])
        if authors:
            score += min(20, len(authors) * 5)
        
        # æ‘˜è¦ (20åˆ†)
        abstract = item.get('abstract', '')
        if abstract:
            if len(abstract) > 500:
                score += 20
            elif len(abstract) > 200:
                score += 15
            else:
                score += 10
        
        # æœŸåˆŠ (15åˆ†)
        if item.get('journal'):
            score += 15
        
        # DOI (15åˆ†)
        if item.get('doi'):
            score += 15
        
        # å¹´ä»½ (10åˆ†)
        if item.get('year') or item.get('publication_year'):
            score += 10
        
        return min(score, 100.0)
    
    def _parse_year(self, year_value: Any) -> Optional[int]:
        """è§£æå¹´ä»½å€¼"""
        if not year_value:
            return None
        
        try:
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæå–æ•°å­—
            if isinstance(year_value, str):
                import re
                year_match = re.search(r'\b(19|20)\d{2}\b', year_value)
                if year_match:
                    return int(year_match.group())
            else:
                return int(year_value)
        except:
            pass
        
        return None
    
    async def _save_imported_data(self, data: List[Dict], project_id: int) -> Dict:
        """ä¿å­˜å¯¼å…¥çš„æ•°æ®åˆ°æ•°æ®åº“"""
        # æ¨¡æ‹Ÿä¿å­˜è¿‡ç¨‹
        saved_count = 0
        skipped_count = 0
        error_count = 0
        details = []
        
        for item in data:
            try:
                # æ¨¡æ‹Ÿé‡å¤æ£€æŸ¥
                is_duplicate = False  # ç®€åŒ–å®ç°
                
                if is_duplicate:
                    skipped_count += 1
                    details.append(f"è·³è¿‡é‡å¤æ–‡çŒ®: {item['title'][:50]}...")
                else:
                    # æ¨¡æ‹Ÿä¿å­˜
                    saved_count += 1
                    details.append(f"å¯¼å…¥æˆåŠŸ: {item['title'][:50]}...")
                    
            except Exception as e:
                error_count += 1
                details.append(f"å¯¼å…¥å¤±è´¥: {str(e)}")
        
        return {
            "saved_count": saved_count,
            "skipped_count": skipped_count,
            "error_count": error_count,
            "details": details[:10]  # åªè¿”å›å‰10æ¡è¯¦æƒ…
        }