"""
é«˜æ€§èƒ½ç½‘ç»œå®¢æˆ·ç«¯ - ä¼˜åŒ–Research Rabbitå’ŒPDFä¸‹è½½æ€§èƒ½
"""

import asyncio
import aiohttp
import aiofiles
import time
from typing import Optional, List, Dict, Union
from dataclasses import dataclass
from urllib.parse import urlparse
import ssl
from concurrent.futures import ThreadPoolExecutor

@dataclass
class NetworkConfig:
    """ç½‘ç»œé…ç½®ä¼˜åŒ–"""
    # è¿æ¥æ± è®¾ç½®
    max_connections: int = 100          # æœ€å¤§è¿æ¥æ•°
    max_connections_per_host: int = 30  # å•ä¸»æœºæœ€å¤§è¿æ¥æ•°
    
    # è¶…æ—¶è®¾ç½® (ä¼˜åŒ–å)
    total_timeout: float = 60.0         # æ€»è¶…æ—¶
    connect_timeout: float = 10.0       # è¿æ¥è¶…æ—¶
    read_timeout: float = 30.0          # è¯»å–è¶…æ—¶
    
    # é‡è¯•è®¾ç½®
    max_retries: int = 3
    retry_delay_base: float = 1.0       # æŒ‡æ•°é€€é¿åŸºæ•°
    
    # å¹¶å‘æ§åˆ¶
    max_concurrent_downloads: int = 20  # æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
    chunk_size: int = 8192             # ä¸‹è½½å—å¤§å°
    
    # HTTP/2ä¼˜åŒ–
    enable_http2: bool = True          # å¯ç”¨HTTP/2
    http2_max_streams: int = 100       # HTTP/2æœ€å¤§æµæ•°
    
    # ç¼“å­˜è®¾ç½®
    enable_dns_cache: bool = True      # DNSç¼“å­˜
    dns_cache_ttl: int = 300          # DNSç¼“å­˜TTL(ç§’)

class HighPerformanceNetworkClient:
    """é«˜æ€§èƒ½ç½‘ç»œå®¢æˆ·ç«¯"""
    
    def __init__(self, config: Optional[NetworkConfig] = None):
        self.config = config or NetworkConfig()
        self.session: Optional[aiohttp.ClientSession] = None
        self.download_semaphore = asyncio.Semaphore(self.config.max_concurrent_downloads)
        self.stats = {
            "requests_made": 0,
            "bytes_downloaded": 0,
            "cache_hits": 0,
            "retry_attempts": 0,
            "average_speed": 0.0
        }
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - åˆ›å»ºä¼˜åŒ–çš„session"""
        await self._create_optimized_session()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """æ¸…ç†èµ„æº"""
        if self.session:
            await self.session.close()
    
    async def _create_optimized_session(self):
        """åˆ›å»ºä¼˜åŒ–çš„HTTPä¼šè¯"""
        # SSLé…ç½®ä¼˜åŒ–
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # è¿æ¥å™¨é…ç½® - æ”¯æŒHTTP/2
        connector = aiohttp.TCPConnector(
            limit=self.config.max_connections,
            limit_per_host=self.config.max_connections_per_host,
            ssl=ssl_context,
            enable_cleanup_closed=True,  # æ¸…ç†å…³é—­çš„è¿æ¥
            use_dns_cache=self.config.enable_dns_cache,
            ttl_dns_cache=self.config.dns_cache_ttl,
            keepalive_timeout=30        # ä¿æŒè¿æ¥30ç§’
        )
        
        # è¶…æ—¶é…ç½®
        timeout = aiohttp.ClientTimeout(
            total=self.config.total_timeout,
            connect=self.config.connect_timeout,
            sock_read=self.config.read_timeout
        )
        
        # åˆ›å»ºä¼šè¯
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "*/*",
                "Accept-Encoding": "gzip, deflate, br",
                "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
                "Connection": "keep-alive"
            },
            # å¯ç”¨å‹ç¼©
            auto_decompress=True,
            # å¯ç”¨cookieæ”¯æŒ
            cookie_jar=aiohttp.CookieJar()
        )
    
    async def download_with_retry(
        self, 
        url: str, 
        max_retries: Optional[int] = None,
        progress_callback=None
    ) -> Optional[bytes]:
        """å¸¦é‡è¯•çš„ä¸‹è½½æ–¹æ³•"""
        max_retries = max_retries or self.config.max_retries
        
        async with self.download_semaphore:
            for attempt in range(max_retries + 1):
                try:
                    start_time = time.time()
                    
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            content = await self._download_with_progress(
                                response, progress_callback
                            )
                            
                            # æ›´æ–°ç»Ÿè®¡
                            download_time = time.time() - start_time
                            self.stats["requests_made"] += 1
                            self.stats["bytes_downloaded"] += len(content)
                            
                            # è®¡ç®—å¹³å‡é€Ÿåº¦ (MB/s)
                            speed = len(content) / (1024 * 1024) / download_time if download_time > 0 else 0
                            self.stats["average_speed"] = (
                                self.stats["average_speed"] * (self.stats["requests_made"] - 1) + speed
                            ) / self.stats["requests_made"]
                            
                            return content
                            
                        elif response.status in [429, 503, 502, 504]:  # å¯é‡è¯•é”™è¯¯
                            if attempt < max_retries:
                                delay = self.config.retry_delay_base * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                                await asyncio.sleep(delay)
                                self.stats["retry_attempts"] += 1
                                continue
                            
                        return None
                        
                except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                    if attempt < max_retries:
                        delay = self.config.retry_delay_base * (2 ** attempt)
                        await asyncio.sleep(delay)
                        self.stats["retry_attempts"] += 1
                        continue
                    
                    print(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
                    return None
            
            return None
    
    async def _download_with_progress(
        self, 
        response: aiohttp.ClientResponse, 
        progress_callback=None
    ) -> bytes:
        """å¸¦è¿›åº¦å›è°ƒçš„ä¸‹è½½"""
        content = bytearray()
        total_size = int(response.headers.get('Content-Length', 0))
        downloaded = 0
        
        async for chunk in response.content.iter_chunked(self.config.chunk_size):
            content.extend(chunk)
            downloaded += len(chunk)
            
            if progress_callback and total_size > 0:
                progress = downloaded / total_size
                await progress_callback(downloaded, total_size, progress)
        
        return bytes(content)
    
    async def batch_download(
        self, 
        urls: List[str],
        progress_callback=None
    ) -> List[Optional[bytes]]:
        """æ‰¹é‡ä¸‹è½½ - å¹¶å‘ä¼˜åŒ–"""
        
        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        tasks = []
        for i, url in enumerate(urls):
            task_progress_callback = None
            if progress_callback:
                task_progress_callback = lambda d, t, p, idx=i: progress_callback(idx, d, t, p)
            
            task = self.download_with_retry(url, progress_callback=task_progress_callback)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        final_results = []
        for result in results:
            if isinstance(result, Exception):
                final_results.append(None)
            else:
                final_results.append(result)
        
        return final_results
    
    async def request_json(
        self, 
        method: str, 
        url: str, 
        **kwargs
    ) -> Optional[Dict]:
        """JSONè¯·æ±‚ - é’ˆå¯¹APIè°ƒç”¨ä¼˜åŒ–"""
        try:
            async with self.session.request(method, url, **kwargs) as response:
                if response.status == 200:
                    return await response.json()
                elif response.status == 429:  # é€Ÿç‡é™åˆ¶
                    retry_after = response.headers.get('Retry-After', '1')
                    await asyncio.sleep(int(retry_after))
                    # é€’å½’é‡è¯•ä¸€æ¬¡
                    return await self.request_json(method, url, **kwargs)
                return None
        except Exception as e:
            print(f"JSONè¯·æ±‚å¤±è´¥ {url}: {e}")
            return None
    
    async def save_to_file(
        self, 
        content: bytes, 
        file_path: str
    ) -> bool:
        """å¼‚æ­¥ä¿å­˜æ–‡ä»¶"""
        try:
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(content)
            return True
        except Exception as e:
            print(f"æ–‡ä»¶ä¿å­˜å¤±è´¥ {file_path}: {e}")
            return False
    
    def get_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            **self.stats,
            "efficiency_rating": self._calculate_efficiency(),
            "recommended_optimization": self._get_optimization_suggestions()
        }
    
    def _calculate_efficiency(self) -> str:
        """è®¡ç®—æ•ˆç‡è¯„çº§"""
        avg_speed = self.stats["average_speed"]
        retry_rate = self.stats["retry_attempts"] / max(self.stats["requests_made"], 1)
        
        if avg_speed > 5.0 and retry_rate < 0.1:
            return "ä¼˜ç§€"
        elif avg_speed > 2.0 and retry_rate < 0.2:
            return "è‰¯å¥½"
        elif avg_speed > 0.5 and retry_rate < 0.3:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦ä¼˜åŒ–"
    
    def _get_optimization_suggestions(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        if self.stats["average_speed"] < 1.0:
            suggestions.append("è€ƒè™‘å¢åŠ å¹¶å‘è¿æ¥æ•°")
            
        if self.stats["retry_attempts"] / max(self.stats["requests_made"], 1) > 0.2:
            suggestions.append("æ£€æŸ¥ç½‘ç»œç¨³å®šæ€§æˆ–å¢åŠ é‡è¯•å»¶è¿Ÿ")
            
        if self.stats["requests_made"] > 100:
            suggestions.append("è€ƒè™‘ä½¿ç”¨è¿æ¥æ± å’ŒDNSç¼“å­˜")
            
        return suggestions or ["æ€§èƒ½è¡¨ç°è‰¯å¥½"]


class OptimizedResearchRabbitClient:
    """ä¼˜åŒ–çš„Research Rabbitå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.base_url = "https://www.researchrabbitapp.com"
        self.network_config = NetworkConfig(
            max_concurrent_downloads=15,  # Research Rabbit APIé™åˆ¶
            max_connections_per_host=20,
            total_timeout=45.0
        )
    
    async def search_papers_fast(
        self, 
        query: str, 
        limit: int = 20
    ) -> Optional[Dict]:
        """ä¼˜åŒ–çš„è®ºæ–‡æœç´¢"""
        
        async with HighPerformanceNetworkClient(self.network_config) as client:
            search_url = f"{self.base_url}/api/search"
            
            payload = {
                "query": query,
                "limit": limit,
                "include_metadata": True
            }
            
            result = await client.request_json("POST", search_url, json=payload)
            
            if result:
                print(f"ğŸ” æœç´¢å®Œæˆ: {query}")
                print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡: {client.get_stats()}")
            
            return result
    
    async def batch_download_pdfs(
        self, 
        pdf_urls: List[str],
        save_directory: str = "/tmp"
    ) -> List[str]:
        """æ‰¹é‡PDFä¸‹è½½ - é«˜æ€§èƒ½ç‰ˆæœ¬"""
        
        async with HighPerformanceNetworkClient(self.network_config) as client:
            
            # æ‰¹é‡ä¸‹è½½
            def progress_callback(task_idx, downloaded, total, progress):
                print(f"ğŸ“¥ ä»»åŠ¡{task_idx}: {downloaded/1024/1024:.1f}MB / {total/1024/1024:.1f}MB ({progress*100:.1f}%)")
            
            results = await client.batch_download(pdf_urls, progress_callback)
            
            # ä¿å­˜æ–‡ä»¶
            saved_files = []
            for i, (url, content) in enumerate(zip(pdf_urls, results)):
                if content:
                    filename = f"{save_directory}/paper_{i}_{int(time.time())}.pdf"
                    if await client.save_to_file(content, filename):
                        saved_files.append(filename)
            
            print(f"ğŸ’¾ ä¿å­˜å®Œæˆ: {len(saved_files)}/{len(pdf_urls)} ä¸ªæ–‡ä»¶")
            print(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡: {client.get_stats()}")
            
            return saved_files


# ä½¿ç”¨ç¤ºä¾‹å’Œæ€§èƒ½å¯¹æ¯”
async def performance_comparison_demo():
    """æ€§èƒ½å¯¹æ¯”æ¼”ç¤º"""
    print("ğŸš€ ç½‘ç»œæ€§èƒ½ä¼˜åŒ–å¯¹æ¯”æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•URL
    test_urls = [
        "https://arxiv.org/pdf/1706.03762.pdf",  # Transformer
        "https://arxiv.org/pdf/1810.04805.pdf",  # BERT
        "https://arxiv.org/pdf/2005.14165.pdf",  # GPT-3
    ]
    
    # 1. ä¼˜åŒ–å‰ (ä¼ ç»Ÿaiohttp)
    print("ğŸ“Š æµ‹è¯•1: ä¼ ç»Ÿaiohttpæ–¹æ³•")
    start_time = time.time()
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        for url in test_urls:
            task = session.get(url)
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        contents = []
        for resp in responses:
            if not isinstance(resp, Exception):
                content = await resp.read()
                contents.append(content)
                resp.close()
    
    traditional_time = time.time() - start_time
    print(f"â±ï¸ ä¼ ç»Ÿæ–¹æ³•è€—æ—¶: {traditional_time:.2f}ç§’")
    
    # 2. ä¼˜åŒ–å (é«˜æ€§èƒ½å®¢æˆ·ç«¯)
    print("\\nğŸ“Š æµ‹è¯•2: é«˜æ€§èƒ½ç½‘ç»œå®¢æˆ·ç«¯")
    start_time = time.time()
    
    async with HighPerformanceNetworkClient() as client:
        optimized_results = await client.batch_download(test_urls)
    
    optimized_time = time.time() - start_time
    print(f"â±ï¸ ä¼˜åŒ–æ–¹æ³•è€—æ—¶: {optimized_time:.2f}ç§’")
    
    # æ€§èƒ½æå‡è®¡ç®—
    if optimized_time > 0:
        improvement = (traditional_time - optimized_time) / optimized_time * 100
        print(f"\\nğŸ¯ æ€§èƒ½æå‡: {improvement:.1f}%")
        print(f"ğŸ“ˆ æ•ˆç‡æå‡: {traditional_time/optimized_time:.1f}x")
    
    return {
        "traditional_time": traditional_time,
        "optimized_time": optimized_time,
        "improvement_percentage": improvement if optimized_time > 0 else 0
    }

if __name__ == "__main__":
    asyncio.run(performance_comparison_demo())