"""
å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†å¼•æ“ - æ ¸å¿ƒå®ç°
æ”¯æŒ200-500ç¯‡æ–‡çŒ®çš„é«˜æ€§èƒ½æ‰¹é‡å¤„ç†
"""

import asyncio
import time
import psutil
import multiprocessing
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
from loguru import logger
import json
from enum import Enum

from app.models.literature import Literature, LiteratureSegment
from app.models.project import Project
from app.services.multi_model_ai_service import MultiModelAIService
from app.services.pdf_processor import PDFProcessor
from app.services.stream_progress_service import StreamProgressService
from app.core.database import SessionLocal
from app.utils.async_limiter import AsyncLimiter


class ProcessingPhase(Enum):
    """å¤„ç†é˜¶æ®µ"""
    INITIALIZATION = "initialization"
    PDF_PROCESSING = "pdf_processing"
    AI_ANALYSIS = "ai_analysis"
    STRUCTURE_GENERATION = "structure_generation"
    DATABASE_OPERATIONS = "database_operations"
    COMPLETED = "completed"


@dataclass
class ProcessingMetrics:
    """å¤„ç†æŒ‡æ ‡"""
    start_time: float
    end_time: Optional[float] = None
    memory_peak: float = 0.0
    cpu_peak: float = 0.0
    tokens_used: int = 0
    api_calls: int = 0
    throughput: float = 0.0
    error_count: int = 0
    success_count: int = 0


class IntelligentResourceManager:
    """æ™ºèƒ½èµ„æºç®¡ç†å™¨"""

    def __init__(self):
        self.memory_monitor_interval = 5.0
        self.cpu_monitor_interval = 1.0
        self._monitoring_active = False
        self._current_metrics = ProcessingMetrics(start_time=time.time())

    async def calculate_optimal_concurrency(self,
                                          literature_count: int,
                                          processing_type: str = "comprehensive") -> Dict[str, Any]:
        """åŠ¨æ€è®¡ç®—æœ€ä¼˜å¹¶å‘å‚æ•°"""

        # è·å–ç³»ç»Ÿèµ„æºçŠ¶æ€
        memory = psutil.virtual_memory()
        cpu_count = multiprocessing.cpu_count()
        available_memory_gb = memory.available / (1024**3)
        cpu_percent = psutil.cpu_percent(interval=1.0)

        logger.info(f"ç³»ç»Ÿèµ„æºè¯„ä¼° - å¯ç”¨å†…å­˜: {available_memory_gb:.1f}GB, CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")

        # PDFå¤„ç†ï¼šå†…å­˜å¯†é›†å‹ï¼Œæ¯ä¸ªä»»åŠ¡çº¦500MB
        base_pdf_concurrent = min(
            int(available_memory_gb / 0.5),  # åŸºäºå†…å­˜
            cpu_count,                       # åŸºäºCPUæ ¸å¿ƒ
            15                              # æœ€å¤§å®‰å…¨é™åˆ¶
        )

        # AIè°ƒç”¨ï¼šAPIé™åˆ¶ä¸ºä¸»ï¼Œå†…å­˜å ç”¨è¾ƒå°‘
        base_ai_concurrent = min(
            30,                             # APIé™åˆ¶
            int(literature_count / 8),      # åŸºäºæ–‡çŒ®æ•°é‡
            base_pdf_concurrent * 2         # AIå¤„ç†æ¯”PDFå¿«
        )

        # æ ¹æ®ç³»ç»Ÿè´Ÿè½½åŠ¨æ€è°ƒæ•´
        load_factor = self._calculate_load_factor(cpu_percent, memory.percent)

        pdf_concurrent = max(1, int(base_pdf_concurrent * load_factor))
        ai_concurrent = max(1, int(base_ai_concurrent * load_factor))

        # æ‰¹æ¬¡å¤§å°åŸºäºå¹¶å‘æ•°å’Œå†…å­˜æƒ…å†µ
        optimal_batch_size = min(
            50,  # æœ€å¤§æ‰¹æ¬¡
            max(10, pdf_concurrent * 3),  # åŸºäºå¤„ç†èƒ½åŠ›
            int(available_memory_gb * 2)  # åŸºäºå†…å­˜
        )

        # ä¼°ç®—å¤„ç†æ—¶é—´
        estimated_duration = self._estimate_processing_time(
            literature_count, pdf_concurrent, ai_concurrent
        )

        config = {
            "pdf_processing_concurrent": pdf_concurrent,
            "ai_analysis_concurrent": ai_concurrent,
            "batch_size": optimal_batch_size,
            "memory_per_task_mb": 512,
            "estimated_duration_seconds": estimated_duration,
            "load_factor": load_factor,
            "system_info": {
                "available_memory_gb": available_memory_gb,
                "cpu_count": cpu_count,
                "cpu_utilization": cpu_percent,
                "memory_utilization": memory.percent
            }
        }

        logger.info(f"è®¡ç®—å‡ºæœ€ä¼˜é…ç½®: PDFå¹¶å‘={pdf_concurrent}, AIå¹¶å‘={ai_concurrent}, æ‰¹æ¬¡å¤§å°={optimal_batch_size}")
        return config

    def _calculate_load_factor(self, cpu_percent: float, memory_percent: float) -> float:
        """è®¡ç®—è´Ÿè½½è°ƒæ•´å› å­"""
        if cpu_percent > 80 or memory_percent > 85:
            return 0.5  # é«˜è´Ÿè½½ï¼Œå‡å°‘50%
        elif cpu_percent > 60 or memory_percent > 70:
            return 0.7  # ä¸­ç­‰è´Ÿè½½ï¼Œå‡å°‘30%
        elif cpu_percent < 30 and memory_percent < 50:
            return 1.2  # ä½è´Ÿè½½ï¼Œå¢åŠ 20%
        else:
            return 1.0  # æ­£å¸¸è´Ÿè½½

    def _estimate_processing_time(self, count: int, pdf_conc: int, ai_conc: int) -> int:
        """ä¼°ç®—å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰"""
        # PDFå¤„ç†ï¼šå¹³å‡æ¯ç¯‡30ç§’ï¼ŒAIåˆ†æï¼šå¹³å‡æ¯ç¯‡10ç§’
        pdf_time = (count * 30) / pdf_conc
        ai_time = (count * 10) / ai_conc

        # è€ƒè™‘æµæ°´çº¿å¹¶è¡Œï¼Œå–è¾ƒé•¿æ—¶é—´çš„85%ï¼ŒåŠ ä¸Šåˆå§‹åŒ–å’Œæ”¶å°¾æ—¶é—´
        pipeline_time = max(pdf_time, ai_time) * 0.85 + 60
        return int(pipeline_time)

    async def start_monitoring(self, session_id: str):
        """å¼€å§‹èµ„æºç›‘æ§"""
        self._monitoring_active = True
        self._current_metrics = ProcessingMetrics(start_time=time.time())

        # å¯åŠ¨åå°ç›‘æ§ä»»åŠ¡
        asyncio.create_task(self._monitor_resources(session_id))

    async def _monitor_resources(self, session_id: str):
        """åå°èµ„æºç›‘æ§ä»»åŠ¡"""
        try:
            while self._monitoring_active:
                # æ›´æ–°èµ„æºä½¿ç”¨å³°å€¼
                memory = psutil.virtual_memory()
                cpu_percent = psutil.cpu_percent()

                self._current_metrics.memory_peak = max(
                    self._current_metrics.memory_peak,
                    memory.used / (1024**3)
                )
                self._current_metrics.cpu_peak = max(
                    self._current_metrics.cpu_peak,
                    cpu_percent
                )

                await asyncio.sleep(self.memory_monitor_interval)

        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.warning(f"èµ„æºç›‘æ§å¼‚å¸¸: {e}")

    def stop_monitoring(self) -> ProcessingMetrics:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›æŒ‡æ ‡"""
        self._monitoring_active = False
        self._current_metrics.end_time = time.time()
        return self._current_metrics


class FaultToleranceManager:
    """å®¹é”™å’Œæ¢å¤ç®¡ç†å™¨"""

    def __init__(self):
        self.checkpoints = {}
        self.retry_delays = [1, 2, 4, 8, 16]  # æŒ‡æ•°é€€é¿

    async def process_with_fault_tolerance(self,
                                         processing_func,
                                         literature_item: Literature,
                                         session_id: str,
                                         max_retries: int = 3) -> Dict[str, Any]:
        """å¸¦å®¹é”™æœºåˆ¶çš„å¤„ç†æ‰§è¡Œ"""

        checkpoint_key = f"{session_id}:lit:{literature_item.id if hasattr(literature_item, 'id') else 'batch'}"

        # æ£€æŸ¥æ£€æŸ¥ç‚¹
        if checkpoint_key in self.checkpoints:
            logger.debug(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: {getattr(literature_item, 'title', 'batch')[:50]}")
            return self.checkpoints[checkpoint_key]

        # æ‰§è¡Œå¤„ç†ï¼Œå¸¦é‡è¯•æœºåˆ¶
        last_exception = None

        for attempt in range(max_retries + 1):
            try:
                result = await processing_func(literature_item)

                # æˆåŠŸæ—¶ä¿å­˜æ£€æŸ¥ç‚¹
                self.checkpoints[checkpoint_key] = result
                return result

            except Exception as e:
                last_exception = e

                if attempt == max_retries:
                    logger.error(f"å¤„ç†æœ€ç»ˆå¤±è´¥: {getattr(literature_item, 'title', 'batch')[:50]} - {e}")
                    break

                # ç­‰å¾…é‡è¯•
                delay = self.retry_delays[min(attempt, len(self.retry_delays) - 1)]
                logger.warning(f"å¤„ç†å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_retries}): {e}")
                await asyncio.sleep(delay)

        return {
            "success": False,
            "literature_id": getattr(literature_item, 'id', None),
            "title": getattr(literature_item, 'title', 'Unknown')[:100],
            "error": str(last_exception),
            "attempts": max_retries + 1,
            "fault_tolerant": True
        }

    async def graceful_degradation(self,
                                 failed_items: List[Literature],
                                 session_id: str) -> Dict[str, Any]:
        """ä¼˜é›…é™çº§å¤„ç†"""
        logger.info(f"å¯åŠ¨ä¼˜é›…é™çº§ï¼Œå¤„ç† {len(failed_items)} ä¸ªå¤±è´¥é¡¹")

        simplified_results = []

        for item in failed_items:
            try:
                # åˆ›å»ºåŸºæœ¬è®°å½•ï¼ˆä»…ä½¿ç”¨ç°æœ‰ä¿¡æ¯ï¼‰
                result = await self._create_basic_literature_record(item)
                simplified_results.append(result)

            except Exception as e:
                logger.error(f"é™çº§å¤„ç†ä¹Ÿå¤±è´¥: {item.title[:50]} - {e}")
                simplified_results.append({
                    "success": False,
                    "literature_id": item.id,
                    "title": item.title[:100],
                    "degraded": True,
                    "error": str(e)
                })

        success_count = len([r for r in simplified_results if r.get("success", False)])

        return {
            "degradation_applied": True,
            "total_items": len(failed_items),
            "processed_count": success_count,
            "still_failed_count": len(failed_items) - success_count,
            "success_rate": success_count / len(failed_items) if failed_items else 0,
            "results": simplified_results
        }

    async def _create_basic_literature_record(self, literature: Literature) -> Dict[str, Any]:
        """åˆ›å»ºåŸºæœ¬æ–‡çŒ®è®°å½•"""
        # ä½¿ç”¨ç°æœ‰çš„æ ‡é¢˜ã€æ‘˜è¦ã€ä½œè€…ä¿¡æ¯åˆ›å»ºç®€åŒ–æ®µè½
        segments = []

        if literature.title:
            segments.append({
                "segment_type": "title",
                "content": literature.title,
                "confidence": 0.9
            })

        if literature.abstract:
            segments.append({
                "segment_type": "abstract",
                "content": literature.abstract,
                "confidence": 0.8
            })

        if literature.authors:
            author_text = f"ä½œè€…: {', '.join(literature.authors)}"
            segments.append({
                "segment_type": "authors",
                "content": author_text,
                "confidence": 0.9
            })

        return {
            "success": True,
            "literature_id": literature.id,
            "title": literature.title[:100],
            "segments_created": len(segments),
            "segments": segments,
            "processing_method": "degraded_basic_info",
            "degraded": True
        }


class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""

    def __init__(self):
        self.progress_service = StreamProgressService()
        self.session_progress = {}

    async def update_progress(self, session_id: str, progress: int, message: str, details: Optional[Dict] = None):
        """æ›´æ–°å¤„ç†è¿›åº¦"""
        try:
            self.session_progress[session_id] = {
                "progress": progress,
                "message": message,
                "timestamp": datetime.utcnow().isoformat(),
                "details": details or {}
            }

            # é€šè¿‡WebSocketå¹¿æ’­è¿›åº¦æ›´æ–°
            await self.progress_service.broadcast_task_update(0, {
                "type": "massive_processing_progress",
                "session_id": session_id,
                "progress": progress,
                "message": message,
                "details": details
            })

            logger.info(f"ä¼šè¯ {session_id} è¿›åº¦: {progress}% - {message}")

        except Exception as e:
            logger.warning(f"è¿›åº¦æ›´æ–°å¤±è´¥: {e}")

    def get_progress(self, session_id: str) -> Optional[Dict]:
        """è·å–ä¼šè¯è¿›åº¦"""
        return self.session_progress.get(session_id)


class DistributedBatchProcessor:
    """åˆ†å¸ƒå¼æ‰¹æ¬¡å¤„ç†å™¨"""

    def __init__(self, resource_manager: IntelligentResourceManager):
        self.resource_manager = resource_manager
        self.progress_tracker = ProgressTracker()
        self.pdf_processor = PDFProcessor()
        self.ai_service = MultiModelAIService()

    async def process_literature_pipeline(self,
                                        literature_batch: List[Literature],
                                        processing_config: Dict[str, Any],
                                        session_id: str) -> Dict[str, Any]:
        """æ–‡çŒ®å¤„ç†æµæ°´çº¿"""

        logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡: {session_id}, æ–‡çŒ®æ•°é‡: {len(literature_batch)}")

        # åˆ›å»ºä¿¡å·é‡æ± 
        pdf_semaphore = asyncio.Semaphore(processing_config["pdf_processing_concurrent"])
        ai_semaphore = asyncio.Semaphore(processing_config["ai_analysis_concurrent"])

        # åˆå§‹åŒ–ç»“æœ
        batch_results = {
            "session_id": session_id,
            "total_literature": len(literature_batch),
            "successful": 0,
            "failed": 0,
            "processing_details": [],
            "performance_metrics": {},
            "phase_results": {}
        }

        try:
            # é˜¶æ®µ1: å¹¶è¡ŒPDFå¤„ç†
            await self.progress_tracker.update_progress(
                session_id, 10, "å¼€å§‹PDFå¤„ç†é˜¶æ®µ",
                {"phase": "pdf_processing", "total": len(literature_batch)}
            )

            pdf_results = await self._parallel_pdf_processing(
                literature_batch, pdf_semaphore, session_id
            )
            batch_results["phase_results"]["pdf_processing"] = pdf_results

            # é˜¶æ®µ2: å¹¶è¡ŒAIåˆ†æ
            await self.progress_tracker.update_progress(
                session_id, 40, "å¼€å§‹AIåˆ†æé˜¶æ®µ",
                {"phase": "ai_analysis", "pdf_completed": len([r for r in pdf_results if r["success"]])}
            )

            successful_pdf_results = [r for r in pdf_results if r["success"]]
            ai_results = await self._parallel_ai_analysis(
                successful_pdf_results, ai_semaphore, session_id
            )
            batch_results["phase_results"]["ai_analysis"] = ai_results

            # é˜¶æ®µ3: ç»“æ„åŒ–æ•°æ®ç”Ÿæˆ
            await self.progress_tracker.update_progress(
                session_id, 70, "ç”Ÿæˆç»“æ„åŒ–æ•°æ®",
                {"phase": "structure_generation", "ai_completed": len([r for r in ai_results if r["success"]])}
            )

            successful_ai_results = [r for r in ai_results if r["success"]]
            structure_results = await self._parallel_structure_generation(
                successful_ai_results, ai_semaphore, session_id
            )
            batch_results["phase_results"]["structure_generation"] = structure_results

            # é˜¶æ®µ4: æ‰¹é‡æ•°æ®åº“ä¿å­˜
            await self.progress_tracker.update_progress(
                session_id, 90, "ä¿å­˜å¤„ç†ç»“æœ",
                {"phase": "database_operations", "structures_completed": len(structure_results)}
            )

            save_results = await self._batch_database_operations(
                structure_results, session_id
            )
            batch_results["phase_results"]["database_operations"] = save_results

            # ç»Ÿè®¡æœ€ç»ˆç»“æœ
            batch_results["successful"] = len([r for r in save_results if r.get("success", False)])
            batch_results["failed"] = len(literature_batch) - batch_results["successful"]
            batch_results["processing_details"] = save_results

            await self.progress_tracker.update_progress(
                session_id, 100, "æ‰¹æ¬¡å¤„ç†å®Œæˆ",
                {
                    "successful": batch_results["successful"],
                    "failed": batch_results["failed"],
                    "success_rate": batch_results["successful"] / len(literature_batch)
                }
            )

            return {"success": True, "results": batch_results}

        except Exception as e:
            logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "partial_results": batch_results
            }

    async def _parallel_pdf_processing(self,
                                     literature_batch: List[Literature],
                                     semaphore: asyncio.Semaphore,
                                     session_id: str) -> List[Dict[str, Any]]:
        """å¹¶è¡ŒPDFå¤„ç†"""

        async def process_single_pdf(literature: Literature):
            async with semaphore:
                try:
                    if not literature.pdf_path and not literature.pdf_url:
                        # æ²¡æœ‰PDFçš„æ–‡çŒ®ï¼Œè·³è¿‡PDFå¤„ç†
                        return {
                            "success": True,
                            "literature_id": literature.id,
                            "title": literature.title[:100],
                            "pdf_processed": False,
                            "content": {"text_content": literature.abstract or ""},
                            "processing_method": "abstract_only"
                        }

                    # å¤„ç†PDF
                    pdf_path = literature.pdf_path or literature.pdf_url
                    result = await self.pdf_processor.process_pdf(pdf_path)

                    if result["success"]:
                        return {
                            "success": True,
                            "literature_id": literature.id,
                            "title": literature.title[:100],
                            "pdf_processed": True,
                            "content": result["content"],
                            "processing_method": "pdf_extraction",
                            "metadata": result.get("metadata", {})
                        }
                    else:
                        # PDFå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨æ‘˜è¦
                        return {
                            "success": True,
                            "literature_id": literature.id,
                            "title": literature.title[:100],
                            "pdf_processed": False,
                            "content": {"text_content": literature.abstract or ""},
                            "processing_method": "fallback_abstract",
                            "pdf_error": result.get("error", "Unknown error")
                        }

                except Exception as e:
                    logger.error(f"PDFå¤„ç†å¼‚å¸¸: {literature.title[:50]} - {e}")
                    return {
                        "success": False,
                        "literature_id": literature.id,
                        "title": literature.title[:100],
                        "error": str(e),
                        "processing_method": "pdf_processing"
                    }

        # å¹¶è¡Œæ‰§è¡ŒPDFå¤„ç†ä»»åŠ¡
        tasks = [process_single_pdf(lit) for lit in literature_batch]
        return await asyncio.gather(*tasks, return_exceptions=False)

    async def _parallel_ai_analysis(self,
                                  pdf_results: List[Dict],
                                  semaphore: asyncio.Semaphore,
                                  session_id: str) -> List[Dict[str, Any]]:
        """å¹¶è¡ŒAIåˆ†æ"""

        async def analyze_single_literature(pdf_result: Dict):
            async with semaphore:
                try:
                    literature_id = pdf_result["literature_id"]
                    content = pdf_result["content"]["text_content"]

                    if not content.strip():
                        return {
                            "success": False,
                            "literature_id": literature_id,
                            "title": pdf_result.get("title", ""),
                            "error": "æ²¡æœ‰å¯åˆ†æçš„å†…å®¹",
                            "processing_method": "ai_analysis"
                        }

                    # ä½¿ç”¨å¤šæ¨¡å‹AIæœåŠ¡è¿›è¡Œåˆ†æ
                    analysis_result = await self.ai_service.analyze_literature_with_multiple_models(
                        content[:4000],  # é™åˆ¶é•¿åº¦é¿å…tokenè¶…é™
                        analysis_type="comprehensive",
                        use_ensemble=True
                    )

                    if analysis_result["success"]:
                        return {
                            "success": True,
                            "literature_id": literature_id,
                            "title": pdf_result.get("title", ""),
                            "ai_analysis": analysis_result,
                            "original_content": content[:1000],  # ä¿å­˜éƒ¨åˆ†åŸå§‹å†…å®¹ç”¨äºç»“æ„åŒ–
                            "processing_method": "ai_comprehensive_analysis"
                        }
                    else:
                        return {
                            "success": False,
                            "literature_id": literature_id,
                            "title": pdf_result.get("title", ""),
                            "error": analysis_result.get("error", "AIåˆ†æå¤±è´¥"),
                            "processing_method": "ai_analysis"
                        }

                except Exception as e:
                    logger.error(f"AIåˆ†æå¼‚å¸¸: {pdf_result.get('title', '')[:50]} - {e}")
                    return {
                        "success": False,
                        "literature_id": pdf_result["literature_id"],
                        "title": pdf_result.get("title", ""),
                        "error": str(e),
                        "processing_method": "ai_analysis"
                    }

        tasks = [analyze_single_literature(result) for result in pdf_results]
        return await asyncio.gather(*tasks, return_exceptions=False)

    async def _parallel_structure_generation(self,
                                           ai_results: List[Dict],
                                           semaphore: asyncio.Semaphore,
                                           session_id: str) -> List[Dict[str, Any]]:
        """å¹¶è¡Œç»“æ„åŒ–æ•°æ®ç”Ÿæˆ"""

        async def generate_structure(ai_result: Dict):
            async with semaphore:
                try:
                    literature_id = ai_result["literature_id"]
                    analysis = ai_result["ai_analysis"]

                    # ä»AIåˆ†æç»“æœä¸­æå–ç»“æ„åŒ–æ®µè½
                    segments = []

                    if "ensemble_result" in analysis:
                        content = analysis["ensemble_result"]["content"]

                        # ç®€å•çš„æ®µè½åˆ†å‰²å’Œåˆ†ç±»
                        paragraphs = content.split('\n\n')

                        for i, paragraph in enumerate(paragraphs):
                            if len(paragraph.strip()) > 50:  # è¿‡æ»¤çŸ­æ®µè½
                                segments.append({
                                    "segment_type": self._classify_segment_type(paragraph),
                                    "content": paragraph.strip(),
                                    "order": i,
                                    "confidence": analysis["ensemble_result"].get("confidence", 0.8),
                                    "source": "ai_analysis_segmentation"
                                })

                    # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ®µè½ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
                    if len(segments) < 2 and "original_content" in ai_result:
                        segments.append({
                            "segment_type": "summary",
                            "content": ai_result["original_content"],
                            "order": 0,
                            "confidence": 0.6,
                            "source": "original_content_fallback"
                        })

                    return {
                        "success": True,
                        "literature_id": literature_id,
                        "title": ai_result.get("title", ""),
                        "segments": segments,
                        "processing_method": "structure_generation",
                        "segment_count": len(segments)
                    }

                except Exception as e:
                    logger.error(f"ç»“æ„åŒ–ç”Ÿæˆå¼‚å¸¸: {ai_result.get('title', '')[:50]} - {e}")
                    return {
                        "success": False,
                        "literature_id": ai_result["literature_id"],
                        "title": ai_result.get("title", ""),
                        "error": str(e),
                        "processing_method": "structure_generation"
                    }

        tasks = [generate_structure(result) for result in ai_results]
        return await asyncio.gather(*tasks, return_exceptions=False)

    def _classify_segment_type(self, paragraph: str) -> str:
        """ç®€å•çš„æ®µè½ç±»å‹åˆ†ç±»"""
        paragraph_lower = paragraph.lower()

        if any(keyword in paragraph_lower for keyword in ["method", "approach", "technique", "methodology"]):
            return "methodology"
        elif any(keyword in paragraph_lower for keyword in ["result", "finding", "outcome", "conclusion"]):
            return "results"
        elif any(keyword in paragraph_lower for keyword in ["introduction", "background", "overview"]):
            return "introduction"
        elif any(keyword in paragraph_lower for keyword in ["discussion", "analysis", "interpretation"]):
            return "discussion"
        else:
            return "general"

    async def _batch_database_operations(self,
                                       structure_results: List[Dict],
                                       session_id: str) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ•°æ®åº“æ“ä½œ"""

        db = SessionLocal()
        final_results = []

        try:
            for result in structure_results:
                try:
                    if not result["success"]:
                        final_results.append(result)
                        continue

                    literature_id = result["literature_id"]
                    segments = result["segments"]

                    # è·å–æ–‡çŒ®å¯¹è±¡
                    literature = db.query(Literature).filter(Literature.id == literature_id).first()
                    if not literature:
                        final_results.append({
                            "success": False,
                            "literature_id": literature_id,
                            "error": "æ–‡çŒ®ä¸å­˜åœ¨",
                            "processing_method": "database_operations"
                        })
                        continue

                    # åˆ é™¤æ—§çš„æ®µè½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    db.query(LiteratureSegment).filter(
                        LiteratureSegment.literature_id == literature_id
                    ).delete()

                    # åˆ›å»ºæ–°çš„æ®µè½
                    segment_objects = []
                    for segment_data in segments:
                        segment = LiteratureSegment(
                            literature_id=literature_id,
                            segment_type=segment_data["segment_type"],
                            content=segment_data["content"],
                            order=segment_data.get("order", 0),
                            extraction_confidence=segment_data.get("confidence", 0.8),
                            structured_data={
                                "source": segment_data.get("source", "massive_processing"),
                                "session_id": session_id,
                                "processing_timestamp": datetime.utcnow().isoformat()
                            }
                        )
                        segment_objects.append(segment)
                        db.add(segment)

                    # æ›´æ–°æ–‡çŒ®çŠ¶æ€
                    literature.is_parsed = True
                    literature.parsing_status = "completed"
                    literature.parsed_content = " ".join([s["content"] for s in segments])

                    db.commit()

                    final_results.append({
                        "success": True,
                        "literature_id": literature_id,
                        "title": result.get("title", ""),
                        "segments_created": len(segment_objects),
                        "processing_method": "database_operations_success"
                    })

                except Exception as e:
                    db.rollback()
                    logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {result.get('title', '')[:50]} - {e}")
                    final_results.append({
                        "success": False,
                        "literature_id": result.get("literature_id"),
                        "title": result.get("title", ""),
                        "error": str(e),
                        "processing_method": "database_operations"
                    })

        finally:
            db.close()

        return final_results


class MassiveLiteratureProcessor:
    """å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†å¼•æ“ä¸»æ§åˆ¶å™¨"""

    def __init__(self):
        self.resource_manager = IntelligentResourceManager()
        self.fault_manager = FaultToleranceManager()
        self.progress_tracker = ProgressTracker()

    async def process_massive_literature(self,
                                       literature_list: List[Literature],
                                       target_count: int = 200,
                                       processing_config: Optional[Dict] = None) -> Dict[str, Any]:
        """å¤§è§„æ¨¡æ–‡çŒ®å¤„ç†ä¸»å…¥å£"""

        session_id = f"massive_{int(time.time())}_{len(literature_list)}"

        try:
            logger.info(f"å¯åŠ¨å¤§è§„æ¨¡å¤„ç†ä¼šè¯: {session_id}, ç›®æ ‡æ–‡çŒ®æ•°: {target_count}")

            # 1. é™åˆ¶å’Œé¢„å¤„ç†
            actual_literature = literature_list[:target_count]

            await self.progress_tracker.update_progress(
                session_id, 5, f"åˆå§‹åŒ–å¤„ç†ï¼Œæ–‡çŒ®æ•°é‡: {len(actual_literature)}"
            )

            # 2. è®¡ç®—æœ€ä¼˜é…ç½®
            optimal_config = await self.resource_manager.calculate_optimal_concurrency(
                len(actual_literature), "comprehensive"
            )

            # 3. å¯åŠ¨èµ„æºç›‘æ§
            await self.resource_manager.start_monitoring(session_id)

            # 4. åˆ†æ‰¹å¤„ç†ç­–ç•¥
            batch_size = optimal_config["batch_size"]
            literature_batches = [
                actual_literature[i:i + batch_size]
                for i in range(0, len(actual_literature), batch_size)
            ]

            logger.info(f"åˆ†æ‰¹ç­–ç•¥: {len(literature_batches)} ä¸ªæ‰¹æ¬¡, æ¯æ‰¹ {batch_size} ç¯‡")

            await self.progress_tracker.update_progress(
                session_id, 10, f"å¼€å§‹åˆ†æ‰¹å¤„ç†ï¼Œå…± {len(literature_batches)} ä¸ªæ‰¹æ¬¡",
                {"batches": len(literature_batches), "batch_size": batch_size}
            )

            # 5. åˆå§‹åŒ–æ€»ä½“ç»“æœ
            overall_results = {
                "session_id": session_id,
                "total_literature": len(actual_literature),
                "total_batches": len(literature_batches),
                "successful": 0,
                "failed": 0,
                "batch_results": [],
                "failed_literature": [],
                "performance_summary": {},
                "processing_time": 0,
                "configuration": optimal_config
            }

            start_time = time.time()

            # 6. æ‰¹æ¬¡å¹¶è¡Œå¤„ç†ï¼ˆæ§åˆ¶å¹¶å‘åº¦é¿å…è¿‡è½½ï¼‰
            batch_processor = DistributedBatchProcessor(self.resource_manager)
            batch_semaphore = asyncio.Semaphore(2)  # æœ€å¤šåŒæ—¶å¤„ç†2ä¸ªæ‰¹æ¬¡

            async def process_single_batch(batch_idx: int, batch: List[Literature]):
                async with batch_semaphore:
                    batch_session_id = f"{session_id}_batch_{batch_idx}"
                    logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(literature_batches)}")

                    return await self.fault_manager.process_with_fault_tolerance(
                        lambda _: batch_processor.process_literature_pipeline(
                            batch, optimal_config, batch_session_id
                        ),
                        batch,  # ä¼ é€’æ‰¹æ¬¡ä½œä¸º"æ–‡çŒ®"å‚æ•°
                        batch_session_id
                    )

            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
            batch_tasks = [
                process_single_batch(i, batch)
                for i, batch in enumerate(literature_batches)
            ]

            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # 7. æ±‡æ€»æ‰¹æ¬¡ç»“æœ
            failed_literature_items = []

            for i, batch_result in enumerate(batch_results):
                if isinstance(batch_result, Exception):
                    logger.error(f"æ‰¹æ¬¡ {i} å¤„ç†å¼‚å¸¸: {batch_result}")
                    failed_literature_items.extend(literature_batches[i])
                    continue

                if batch_result.get("success") and "results" in batch_result:
                    result_data = batch_result["results"]
                    overall_results["successful"] += result_data["successful"]
                    overall_results["failed"] += result_data["failed"]
                    overall_results["batch_results"].append(result_data)
                else:
                    failed_literature_items.extend(literature_batches[i])

            # 8. ä¼˜é›…é™çº§å¤„ç†å¤±è´¥é¡¹
            if failed_literature_items:
                await self.progress_tracker.update_progress(
                    session_id, 85, f"å¯¹ {len(failed_literature_items)} ä¸ªå¤±è´¥é¡¹å¯åŠ¨é™çº§å¤„ç†"
                )

                degradation_result = await self.fault_manager.graceful_degradation(
                    failed_literature_items, session_id
                )

                overall_results["degradation_result"] = degradation_result
                overall_results["successful"] += degradation_result["processed_count"]
                overall_results["failed_literature"] = [
                    r for r in degradation_result["results"] if not r.get("success", False)
                ]

            # 9. å®Œæˆå¤„ç†å’Œæ€§èƒ½æ€»ç»“
            overall_results["processing_time"] = time.time() - start_time

            # åœæ­¢èµ„æºç›‘æ§
            final_metrics = self.resource_manager.stop_monitoring()
            final_metrics.success_count = overall_results["successful"]
            final_metrics.error_count = overall_results["failed"]
            final_metrics.throughput = overall_results["successful"] / overall_results["processing_time"] if overall_results["processing_time"] > 0 else 0

            overall_results["performance_summary"] = {
                "memory_peak": final_metrics.memory_peak,
                "cpu_peak": final_metrics.cpu_peak,
                "tokens_used": final_metrics.tokens_used,
                "api_calls": final_metrics.api_calls,
                "throughput": final_metrics.throughput,
                "processing_time": overall_results["processing_time"]
            }

            # 10. æœ€ç»ˆè¿›åº¦æ›´æ–°
            success_rate = overall_results["successful"] / overall_results["total_literature"]

            await self.progress_tracker.update_progress(
                session_id, 100, "å¤§è§„æ¨¡å¤„ç†å®Œæˆ",
                {
                    "successful": overall_results["successful"],
                    "failed": overall_results["failed"],
                    "success_rate": success_rate,
                    "throughput": final_metrics.throughput
                }
            )

            logger.info(f"å¤§è§„æ¨¡å¤„ç†å®Œæˆ: {session_id}")
            logger.info(f"å¤„ç†ç»“æœ: {overall_results['successful']}/{overall_results['total_literature']} æˆåŠŸ ({success_rate:.1%})")
            logger.info(f"å¤„ç†æ—¶é—´: {overall_results['processing_time']:.1f} ç§’")
            logger.info(f"å¤„ç†æ•ˆç‡: {final_metrics.throughput:.2f} ç¯‡/ç§’")

            return {
                "success": True,
                "results": overall_results,
                "recommendations": self._generate_recommendations(overall_results)
            }

        except Exception as e:
            logger.error(f"å¤§è§„æ¨¡å¤„ç†å¼•æ“å¼‚å¸¸: {e}")
            return {
                "success": False,
                "error": str(e),
                "session_id": session_id,
                "partial_results": locals().get("overall_results", {})
            }

    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        success_rate = results["successful"] / results["total_literature"]
        processing_time = results["processing_time"]
        throughput = results["performance_summary"].get("throughput", 0)

        # æˆåŠŸç‡åˆ†æ
        if success_rate >= 0.95:
            recommendations.append("âœ… å¤„ç†æˆåŠŸç‡ä¼˜ç§€ (â‰¥95%)")
        elif success_rate >= 0.85:
            recommendations.append("âœ… å¤„ç†æˆåŠŸç‡è‰¯å¥½ (â‰¥85%)")
        elif success_rate >= 0.7:
            recommendations.append("âš ï¸ å¤„ç†æˆåŠŸç‡ä¸­ç­‰ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")
        else:
            recommendations.append("âŒ å¤„ç†æˆåŠŸç‡è¾ƒä½ï¼Œéœ€è¦æ£€æŸ¥ç³»ç»Ÿé…ç½®å’Œèµ„æºåˆ†é…")

        # æ€§èƒ½åˆ†æ
        if throughput >= 1.0:
            recommendations.append(f"âœ… å¤„ç†æ•ˆç‡ä¼˜ç§€: {throughput:.2f} ç¯‡/ç§’")
        elif throughput >= 0.5:
            recommendations.append(f"âœ… å¤„ç†æ•ˆç‡è‰¯å¥½: {throughput:.2f} ç¯‡/ç§’")
        else:
            recommendations.append(f"âš ï¸ å¤„ç†æ•ˆç‡è¾ƒä½: {throughput:.2f} ç¯‡/ç§’ï¼Œå»ºè®®å¢åŠ ç³»ç»Ÿèµ„æº")

        # å†…å­˜å’ŒCPUä½¿ç”¨åˆ†æ
        memory_peak = results["performance_summary"].get("memory_peak", 0)
        cpu_peak = results["performance_summary"].get("cpu_peak", 0)

        if memory_peak > 12:  # >12GB
            recommendations.append("âš ï¸ å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ‰¹æ¬¡å¤§å°æˆ–å¢åŠ å†…å­˜")

        if cpu_peak > 80:  # >80%
            recommendations.append("âš ï¸ CPUä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®é™ä½å¹¶å‘æ•°æˆ–å¢åŠ CPUèµ„æº")

        # é™çº§å¤„ç†åˆ†æ
        if results.get("degradation_result"):
            degraded_count = results["degradation_result"]["processed_count"]
            recommendations.append(f"â„¹ï¸ æœ‰ {degraded_count} ç¯‡æ–‡çŒ®ä½¿ç”¨äº†é™çº§å¤„ç†ï¼Œå»ºè®®æ£€æŸ¥åŸå§‹å¤„ç†å¤±è´¥åŸå› ")

        # æ€»ä½“å»ºè®®
        recommendations.append(f"ğŸ“Š æœ¬æ¬¡å¤„ç†ç»Ÿè®¡: æ€»è®¡ {results['total_literature']} ç¯‡ï¼ŒæˆåŠŸ {results['successful']} ç¯‡ï¼Œç”¨æ—¶ {processing_time:.1f} ç§’")

        return recommendations