"""
æ–‡çŒ®å¤„ç†ç›¸å…³çš„åå°ä»»åŠ¡
"""

import asyncio
import os
from datetime import datetime
from typing import List, Dict, Any, Optional, Callable, Awaitable
from loguru import logger
from sqlalchemy.orm import Session

from app.core.database import SessionLocal
from app.core.config import settings
from app.models.task import Task
from app.services.task_stream_service import TaskStreamService
from app.services.task_cost_tracker import task_cost_tracker
from app.models.project import Project
from app.models.experience import MainExperience
from app.models.literature import Literature, LiteratureSegment
from app.models.user import User
from app.services.literature_collector import EnhancedLiteratureCollector
from app.services.pdf_processor import PDFProcessor
from app.services.research_ai_service import research_ai_service
from app.services.experience_engine import EnhancedExperienceEngine
from app.services.rag_service import RAGService
from app.services.task_orchestrator import TaskOrchestrator
from app.tasks.literature_tasks_helper import create_basic_segments

async def safe_broadcast_update(progress_service, task_id: int, data: dict):
    """å®‰å…¨çš„WebSocketå¹¿æ’­æ›´æ–°"""
    try:
        await progress_service.broadcast_task_update(task_id, data)
    except Exception as ws_error:
        logger.warning(f"WebSocketæ›´æ–°å¤±è´¥ (Task {task_id}): {ws_error}")

def ai_search_batch_task(
    task_id: int,
    query: str,
    max_results: int
):
    """
    AIæ‰¹é‡æœç´¢æ–‡çŒ®çš„åå°ä»»åŠ¡
    """
    import asyncio
    
    try:
        # è¿è¡Œå¼‚æ­¥æœç´¢ä»»åŠ¡
        asyncio.run(ai_search_batch_async(task_id, query, max_results))
    except Exception as e:
        logger.error(f"AIæœç´¢ä»»åŠ¡å¤±è´¥: {e}")
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        db = SessionLocal()
        try:
            task = db.query(Task).filter(Task.id == task_id).first()
            if task:
                task.status = "failed"
                task.error_message = str(e)
                task.completed_at = datetime.utcnow()
                task.progress_percentage = 0
                db.commit()
                
                # å‘é€WebSocketæ›´æ–°
                from app.services.stream_progress_service import StreamProgressService
                progress_service = StreamProgressService()
                # Note: Cannot await in sync function, WebSocket update will be handled by async task
        finally:
            db.close()

async def ai_search_batch_async(
    task_id: int,
    query: str,
    max_results: int
):
    """
    å¼‚æ­¥AIæœç´¢ä»»åŠ¡å®ç°
    """
    db = SessionLocal()
    task_stream = TaskStreamService(db)
    token = None

    try:
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            raise ValueError("ä»»åŠ¡ä¸å­˜åœ¨")

        project = db.query(Project).filter(Project.id == task.project_id).first()
        if not project:
            raise ValueError("é¡¹ç›®ä¸å­˜åœ¨")

        from app.services.research_rabbit_client import ResearchRabbitClient
        from app.services.shared_literature_service import SharedLiteratureService

        literature_service = SharedLiteratureService(db)

        token = task_cost_tracker.activate(task.id, db)
        await task_stream.start_task(task, "ğŸ” æ­£åœ¨ä¸ºæ‚¨å¯»æ‰¾ç›¸å…³ç ”ç©¶æ–‡çŒ®...")

        total_found = 0
        total_added = 0

        async with ResearchRabbitClient() as client:
            await task_stream.update_progress(
                task,
                f"ğŸ“š æ­£åœ¨æœç´¢ä¸»é¢˜ï¼š{query}",
                10,
                {"query": query}
            )

            papers = await client.search_all_papers(query, max_results)
            total_found = len(papers)

            await task_stream.update_progress(
                task,
                f"âœ… å‘ç° {total_found} ç¯‡ç›¸å…³è®ºæ–‡ï¼Œæ­£åœ¨ç­›é€‰ä¼˜è´¨å†…å®¹...",
                30,
                {"found_count": total_found}
            )

            new_papers: List[Dict[str, Any]] = []

            for i, paper in enumerate(papers):
                try:
                    external_ids = paper.get("externalIds") or {}
                    if isinstance(external_ids, str):
                        external_ids = {}

                    doi = external_ids.get("DOI") if isinstance(external_ids, dict) else None
                    arxiv_id = external_ids.get("ArXiv") if isinstance(external_ids, dict) else None
                    title = paper.get("title", "")

                    existing_lit = await literature_service.find_existing_literature(
                        doi=doi,
                        arxiv_id=arxiv_id,
                        title=title
                    )

                    if existing_lit is None:
                        new_papers.append(paper)

                    if (i + 1) % 5 == 0:
                        progress = 30 + int((i + 1) / max(1, len(papers)) * 40)
                        await task_stream.update_progress(
                            task,
                            f"â³ å·²è¯„ä¼° {i + 1}/{len(papers)} ç¯‡è®ºæ–‡",
                            progress,
                            {
                                "processed_count": i + 1,
                                "total_count": len(papers),
                                "eta_minutes": ((len(papers) - (i + 1)) * 2) // 60 + 1
                            }
                        )

                except Exception as item_error:
                    logger.warning(f"å¤„ç†æ–‡çŒ® {i} æ—¶å‡ºé”™: {item_error}")
                    continue

            if new_papers:
                await task_stream.update_progress(
                    task,
                    f"ğŸ“– æ­£åœ¨ä¸ºé¡¹ç›®æ·»åŠ  {len(new_papers)} ç¯‡ä¼˜è´¨è®ºæ–‡...",
                    70,
                    {"new_count": len(new_papers)}
                )

                user = db.query(User).filter(User.id == project.owner_id).first()
                if user:
                    for paper in new_papers:
                        try:
                            await literature_service.add_literature_from_search(
                                user_id=user.id,
                                project_id=task.project_id,
                                paper_data=paper
                            )
                            total_added += 1
                        except Exception as add_error:
                            logger.warning(f"æ·»åŠ æ–‡çŒ®å¤±è´¥: {add_error}")
                            continue

        completion_details = {
            "success": True,
            "total_found": total_found,
            "total_added": total_added,
            "query": query
        }
        await task_stream.complete_task(task, completion_details)
        return completion_details

    except Exception as e:
        logger.error(f"AIæœç´¢ä»»åŠ¡å¤±è´¥: {e}")
        task = db.query(Task).filter(Task.id == task_id).first()
        if task:
            await task_stream.fail_task(task, str(e))
        return {"success": False, "error": str(e)}

    finally:
        if token is not None:
            task_cost_tracker.deactivate(token)
        db.close()

async def start_search_and_build_library_task(
    task_id: int,
    keywords: List[str],
    project_id: int,
    user_id: int,
    config: Dict[str, Any]
):
    """å¯åŠ¨æœç´¢å»ºåº“åŸå­åŒ–ä»»åŠ¡ï¼Œç»Ÿä¸€ç”±TaskStreamServiceæ‰˜ç®¡è¿›åº¦ã€‚"""
    db = SessionLocal()

    try:
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

        project = db.query(Project).filter(
            Project.id == project_id,
            Project.owner_id == user_id
        ).first()
        if not project:
            raise ValueError(f"é¡¹ç›®ä¸å­˜åœ¨æˆ–æ— æƒè®¿é—®: {project_id}")

        user = db.query(User).filter(User.id == user_id).first()
        if not user:
            raise ValueError(f"ç”¨æˆ·ä¸å­˜åœ¨: {user_id}")

        task_stream = TaskStreamService(db)

        async def run_pipeline(progress_callback):
            from app.services.search_and_build_library_service import (
                create_search_and_build_library_service,
            )
            from app.services.search_pipeline import ProcessingConfig

            await progress_callback(
                "å‡†å¤‡æœç´¢å»ºåº“ä¸Šä¸‹æ–‡",
                5,
                {"keywords": keywords, "project_id": project_id},
            )

            processing_config = ProcessingConfig(
                batch_size=config.get("batch_size", 10),
                max_concurrent_downloads=config.get("max_concurrent_downloads", 5),
                max_concurrent_ai_calls=config.get("max_concurrent_ai_calls", 3),
                enable_ai_filtering=config.get("enable_ai_filtering", True),
                enable_pdf_processing=config.get("enable_pdf_processing", True),
                enable_structured_extraction=config.get("enable_structured_extraction", True),
                quality_threshold=config.get("quality_threshold", 6.0),
                max_retries=config.get("max_retries", 3),
                timeout_seconds=config.get("timeout_seconds", 3600),
                max_results=config.get("max_results", 200),
            )

            async def pipeline_progress(step: str, progress: int, details: Dict = None):
                await progress_callback(step, progress, details or {})

            async with create_search_and_build_library_service(db) as service:
                result = await service.execute_full_pipeline(
                    keywords=keywords,
                    project=project,
                    config=processing_config,
                    progress_callback=pipeline_progress,
                )

            if not result.get("success"):
                raise RuntimeError(result.get("error", "æœç´¢å»ºåº“å¤±è´¥"))

            stats = result.get("stats", {})
            completion_details = {
                "success": True,
                "stats": stats,
                "processed_items": result.get("processed_items", 0),
                "processing_time": result.get("processing_time", 0),
            }
            return completion_details

        result = await task_stream.run_with_progress(
            task,
            "åˆå§‹åŒ–æœç´¢å»ºåº“æµæ°´çº¿",
            run_pipeline,
        )

        if result.get("success"):
            await _ensure_project_main_experience(project, db)

        await _schedule_auto_pipeline_next_step(task, db)
        return result

    except Exception as exc:
        logger.error(f"æœç´¢å»ºåº“ä»»åŠ¡å¼‚å¸¸: {exc}")
        raise
    finally:
        db.close()


async def start_literature_collection_task(
    task_id: int,
    keywords: List[str],
    max_count: int,
    sources: List[str]
):
    """å¯åŠ¨æ–‡çŒ®é‡‡é›†ä»»åŠ¡"""
    db = SessionLocal()

    collector: Optional[EnhancedLiteratureCollector] = None

    try:
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

        project = db.query(Project).filter(Project.id == task.project_id).first()
        if not project:
            raise ValueError(f"é¡¹ç›®ä¸å­˜åœ¨: {task.project_id}")

        user = db.query(User).filter(User.id == project.owner_id).first()
        if not user:
            raise ValueError(f"ç”¨æˆ·ä¸å­˜åœ¨: {project.owner_id}")

        task_stream = TaskStreamService(db)
        collector = EnhancedLiteratureCollector()

        async def run_collection(progress_callback):
            await progress_callback(
                "å‡†å¤‡æ–‡çŒ®é‡‡é›†ä¸Šä¸‹æ–‡",
                5,
                {
                    "keywords": keywords,
                    "max_count": max_count,
                    "sources": sources,
                },
            )

            async def collection_progress(step: str, progress: int, details: Dict = None):
                await progress_callback(step, progress, details or {})

            result = await collector.collect_literature(
                keywords=keywords,
                max_count=max_count,
                project_id=project.id,
                task_id=task.id,
            )

            summary = {
                "success": True,
                "total_collected": len(result.get("literature", [])),
                "statistics": result.get("statistics", {}),
            }
            return summary

        result = await task_stream.run_with_progress(
            task,
            "åˆå§‹åŒ–æ–‡çŒ®é‡‡é›†",
            run_collection,
        )

        await _schedule_auto_pipeline_next_step(task, db)
        return result

    except Exception as exc:
        logger.error(f"æ–‡çŒ®é‡‡é›†ä»»åŠ¡å¼‚å¸¸: {exc}")
        raise
    finally:
        db.close()

async def _schedule_auto_pipeline_next_step(task: Task, db: Session) -> None:
    """æ ¹æ®ä»»åŠ¡é…ç½®è°ƒåº¦è‡ªåŠ¨ç ”ç©¶æµæ°´çº¿çš„ä¸‹ä¸€æ­¥ã€‚"""
    try:
        config = task.config or {}
        pipeline_cfg = config.get("auto_pipeline")
        if not pipeline_cfg:
            return

        next_step = pipeline_cfg.get("on_complete")
        if not next_step:
            return

        project = task.project
        owner_id = project.owner_id if project else None
        if owner_id is None:
            logger.warning(f"ä»»åŠ¡ {task.id} ç¼ºå°‘é¡¹ç›®æ‰€æœ‰è€…ä¿¡æ¯ï¼Œæ— æ³•ç»§ç»­è‡ªåŠ¨æµæ°´çº¿")
            return

        orchestrator = TaskOrchestrator(db)
        step_type = next_step.get("type")
        payload = next_step.get("payload", {})

        if step_type == "search_and_build_library":
            keywords = payload.get("keywords") or []
            search_config = payload.get("config") or {}
            orchestrator.trigger_search_pipeline(
                owner_id=owner_id,
                project_id=task.project_id,
                keywords=keywords,
                config=search_config,
            )
        elif step_type == "experience_generation":
            research_question = payload.get("query") or pipeline_cfg.get("query") or "é€šç”¨ç ”ç©¶é—®é¢˜"
            processing_method = payload.get("processing_method") or pipeline_cfg.get("processing_method") or "standard"
            orchestrator.trigger_experience_task(
                owner_id=owner_id,
                project_id=task.project_id,
                research_question=research_question,
                processing_method=processing_method,
            )
    except Exception as pipeline_error:
        logger.error(f"è‡ªåŠ¨ç ”ç©¶æµæ°´çº¿è°ƒåº¦å¤±è´¥: {pipeline_error}")


async def _ensure_project_main_experience(project: Project, db: Session) -> None:
    """ç¡®ä¿é¡¹ç›®å­˜åœ¨ä¸»ç»éªŒï¼Œå¦‚ç¼ºå¤±åˆ™åŸºäºæœ€æ–°ç»“æ„åŒ–æ–‡çŒ®è‡ªåŠ¨ç”Ÿæˆã€‚"""
    try:
        existing = (
            db.query(MainExperience)
            .filter(MainExperience.project_id == project.id)
            .first()
        )
        if existing:
            return

        segments = (
            db.query(LiteratureSegment)
            .join(Literature)
            .filter(Literature.projects.any(id=project.id))
            .limit(500)
            .all()
        )
        if not segments:
            return

        engine = EnhancedExperienceEngine(db)
        await engine.create_main_experiences(project, segments)

    except Exception as exc:
        logger.warning(f"è‡ªåŠ¨ç”Ÿæˆä¸»ç»éªŒå¤±è´¥ (é¡¹ç›® {project.id}): {exc}")

async def start_literature_processing_task(task_id: int):
    """å¯åŠ¨æ–‡çŒ®å¤„ç†ä»»åŠ¡"""
    db = SessionLocal()

    try:
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

        task_stream = TaskStreamService(db)

        async def run_processing(progress_callback):
            project = db.query(Project).filter(Project.id == task.project_id).first()
            if not project:
                raise ValueError(f"é¡¹ç›®ä¸å­˜åœ¨: {task.project_id}")

            unprocessed_literature = db.query(Literature).filter(
                Literature.projects.any(id=task.project_id),
                Literature.is_parsed.is_(False)
            ).all()

            if not unprocessed_literature:
                await progress_callback(
                    "æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡çŒ®",
                    100,
                    {"processed_count": 0, "total_literature": 0},
                )
                return {
                    "success": True,
                    "processed_count": 0,
                    "total_literature": 0,
                    "message": "æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡çŒ®",
                }

            await progress_callback(
                "å‡†å¤‡æ–‡çŒ®å¤„ç†ä¸Šä¸‹æ–‡",
                5,
                {"pending_literature": len(unprocessed_literature)},
            )

            sample_literature = [
                {
                    "title": lit.title,
                    "abstract": lit.abstract or "",
                    "authors": lit.authors or [],
                }
                for lit in unprocessed_literature[:5]
            ]

            template_result = await research_ai_service.generate_structure_template(
                project.research_direction or "é€šç”¨ç§‘ç ”",
                sample_literature,
            )

            template_details = {
                "template_generated": template_result.get("success", False),
                "sample_size": len(sample_literature),
            }

            if template_result.get("success"):
                project.structure_template = template_result.get("template")
                db.commit()
                template_details["template_keys"] = list(
                    (project.structure_template or {}).keys()
                )
            else:
                template_details["error"] = template_result.get("error")

            await progress_callback("ç”Ÿæˆç»“æ„åŒ–æ¨¡æ¿", 10, template_details)

            total_items = len(unprocessed_literature)
            literature_ids = [lit.id for lit in unprocessed_literature]

            concurrency_limit = max(settings.literature_processing_concurrency, 1)
            semaphore = asyncio.Semaphore(concurrency_limit)
            progress_lock = asyncio.Lock()

            progress_state: Dict[str, Any] = {
                "processed": 0,
                "processed_success": 0,
                "fallback_used": 0,
                "failures": []
            }

            async def process_single_literature(literature_id: int):
                async with semaphore:
                    local_db = SessionLocal()
                    used_fallback = False
                    success = False
                    failure_info: Optional[Dict[str, Any]] = None
                    try:
                        literature = (
                            local_db.query(Literature)
                            .filter(Literature.id == literature_id)
                            .first()
                        )
                        if not literature:
                            raise ValueError(f"æ–‡çŒ®ä¸å­˜åœ¨: {literature_id}")

                        project_obj = (
                            local_db.query(Project)
                            .filter(Project.id == task.project_id)
                            .first()
                        )
                        if not project_obj:
                            raise ValueError(f"é¡¹ç›®ä¸å­˜åœ¨: {task.project_id}")

                        logger.info(f"å¹¶è¡Œå¤„ç†æ–‡çŒ®: {literature.title[:50]}...")
                        new_segments = 0

                        if literature.pdf_url or literature.pdf_path:
                            pdf_path = literature.pdf_path or literature.pdf_url
                            if pdf_path and os.path.exists(pdf_path):
                                try:
                                    pdf_processor = PDFProcessor()
                                    result = await pdf_processor.process_pdf_with_segments(
                                        pdf_path,
                                        project_obj.structure_template,
                                    )

                                    if result.get("success"):
                                        segments = result.get("segments", [])
                                        for segment_data in segments:
                                            segment = LiteratureSegment(
                                                literature_id=literature.id,
                                                segment_type=segment_data.get("segment_type", "general"),
                                                content=segment_data.get("content", ""),
                                                page_number=segment_data.get("page_number", 1),
                                                extraction_confidence=segment_data.get("confidence", 0.5),
                                                structured_data={
                                                    "source": "mineru_processing",
                                                    "processor_version": result.get("metadata", {}).get("version", "unknown"),
                                                },
                                            )
                                            local_db.add(segment)
                                            new_segments += 1

                                        literature.is_parsed = True
                                        literature.parsing_status = "completed"
                                        content_payload = result.get("content") or {}
                                        literature.parsed_content = content_payload.get("text_content", "")
                                        success = True
                                    else:
                                        logger.warning(
                                            f"PDFå¤„ç†å¤±è´¥: {result.get('error', 'Unknown error')}"
                                        )
                                        await create_basic_segments(literature, local_db, project_obj)
                                        used_fallback = True
                                        success = True
                                except Exception as processing_exc:
                                    logger.error(f"PDFå¤„ç†å¼‚å¸¸: {processing_exc}")
                                    await create_basic_segments(literature, local_db, project_obj)
                                    used_fallback = True
                                    success = True
                            else:
                                await create_basic_segments(literature, local_db, project_obj)
                                used_fallback = True
                                success = True
                        else:
                            await create_basic_segments(literature, local_db, project_obj)
                            used_fallback = True
                            success = True

                        local_db.commit()

                    except Exception as item_error:
                        local_db.rollback()
                        failure_info = {
                            "literature_id": literature_id,
                            "error": str(item_error),
                        }
                        logger.error(f"å¹¶è¡Œå¤„ç†æ–‡çŒ®å¤±è´¥ {literature_id}: {item_error}")

                        # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
                        persisted = (
                            local_db.query(Literature)
                            .filter(Literature.id == literature_id)
                            .first()
                        )
                        if persisted:
                            persisted.parsing_status = "failed"
                            local_db.commit()
                    finally:
                        local_db.close()

                    async with progress_lock:
                        progress_state["processed"] += 1
                        if success:
                            progress_state["processed_success"] += 1
                        if used_fallback:
                            progress_state["fallback_used"] += 1
                        if failure_info:
                            progress_state["failures"].append(failure_info)

                        progress_pct = 10 + int(
                            (progress_state["processed"] / max(total_items, 1)) * 80
                        )
                        details = {
                            "processed": progress_state["processed"],
                            "total": total_items,
                            "processed_success": progress_state["processed_success"],
                            "fallback_used": progress_state["fallback_used"],
                        }
                        if failure_info:
                            details["last_failure"] = failure_info

                        await progress_callback(
                            f"å¤„ç†æ–‡çŒ® {progress_state['processed']}/{total_items}",
                            min(progress_pct, 95),
                            details,
                        )

            await asyncio.gather(*[process_single_literature(lit_id) for lit_id in literature_ids])

            summary = {
                "success": True,
                "processed_count": progress_state["processed_success"],
                "total_literature": total_items,
                "fallback_used": progress_state["fallback_used"],
                "failures": progress_state["failures"],
            }
            return summary

        return await task_stream.run_with_progress(
            task,
            "åˆå§‹åŒ–æ–‡çŒ®å¤„ç†",
            run_processing,
        )

    except Exception as exc:
        logger.error(f"æ–‡çŒ®å¤„ç†ä»»åŠ¡å¼‚å¸¸: {exc}")
        raise
    finally:
        db.close()

async def start_experience_generation_task(task_id: int, research_question: str):
    """å¯åŠ¨ç»éªŒç”Ÿæˆä»»åŠ¡"""
    db = SessionLocal()

    try:
        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

        task_stream = TaskStreamService(db)

        async def run_experience(progress_callback):
            rag_service = RAGService(db)
            rag_results = await rag_service.search_relevant_segments(
                query=research_question,
                project_id=task.project_id,
                top_k=60,
            )

            segment_ids = [item.get("id") for item in rag_results if item.get("id")]

            if segment_ids:
                segment_order = {seg_id: idx for idx, seg_id in enumerate(segment_ids)}
                literature_segments = (
                    db.query(LiteratureSegment)
                    .filter(LiteratureSegment.id.in_(segment_ids))
                    .all()
                )
                literature_segments.sort(
                    key=lambda segment: segment_order.get(segment.id, len(segment_ids))
                )
            else:
                literature_segments = (
                    db.query(LiteratureSegment)
                    .join(Literature)
                    .filter(Literature.projects.any(id=task.project_id))
                    .all()
                )

            if not literature_segments:
                raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æ–‡çŒ®æ®µè½")

            await progress_callback(
                "å‡†å¤‡ç»éªŒç”Ÿæˆä¸Šä¸‹æ–‡",
                10,
                {
                    "segment_count": len(literature_segments),
                    "research_question": research_question,
                },
            )

            experience_engine = EnhancedExperienceEngine(db)
            result = await experience_engine.run_experience_enhancement(
                task.project_id,
                research_question,
                literature_segments,
                task_id,
            )

            if not result.get("success"):
                raise RuntimeError(result.get("error", "ç»éªŒç”Ÿæˆå¤±è´¥"))

            return result

        return await task_stream.run_with_progress(
            task,
            "å‡†å¤‡ç»éªŒç”Ÿæˆ",
            run_experience,
        )

    except Exception as exc:
        logger.error(f"ç»éªŒç”Ÿæˆä»»åŠ¡å¤±è´¥: {exc}")
        raise
    finally:
        db.close()

async def _perform_indexing(
    db: Session,
    project: Project,
    literature_segments: List[LiteratureSegment],
    progress_callback: Optional[Callable[[str, int, Dict[str, Any]], Awaitable[None]]] = None,
) -> Dict[str, Any]:
    """æ‰§è¡Œç´¢å¼•æ„å»ºæ ¸å¿ƒé€»è¾‘ã€‚"""

    if not literature_segments:
        raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æ–‡çŒ®æ®µè½æ•°æ®ï¼Œè¯·å…ˆå¤„ç†æ–‡çŒ®")

    total_segments = len(literature_segments)
    logger.info(f"å¼€å§‹æ„å»ºé¡¹ç›® {project.id} çš„æ–‡çŒ®ç´¢å¼•ï¼Œå…± {total_segments} ä¸ªæ®µè½")

    for index, _ in enumerate(literature_segments):
        if progress_callback and ((index + 1) % 10 == 0 or index == total_segments - 1):
            progress = int(((index + 1) / total_segments) * 100)
            await progress_callback(
                f"ç´¢å¼•æ„å»º {index + 1}/{total_segments}",
                progress,
                {
                    "processed_segments": index + 1,
                    "total_segments": total_segments,
                },
            )

    project.status = "indexed"
    project.updated_at = datetime.utcnow()
    if hasattr(project, "indexed_at"):
        setattr(project, "indexed_at", datetime.utcnow())
    if hasattr(project, "index_version"):
        current_version = getattr(project, "index_version") or 0
        setattr(project, "index_version", current_version + 1)

    db.commit()

    logger.info(f"é¡¹ç›® {project.id} ç´¢å¼•æ„å»ºå®Œæˆ")
    return {
        "success": True,
        "indexed_segments": total_segments,
    }


async def build_literature_index(project_id: int, task_id: str, user_id: int):
    """æ„å»ºæ–‡çŒ®åº“ç´¢å¼•çš„åå°ä»»åŠ¡ï¼Œå…¼å®¹ä»»åŠ¡ç³»ç»Ÿä¸ç›´æ¥è§¦å‘ä¸¤ç§æ¨¡å¼ã€‚"""
    db = SessionLocal()

    try:
        project = db.query(Project).filter(Project.id == project_id).first()
        if not project:
            raise ValueError(f"é¡¹ç›®ä¸å­˜åœ¨: {project_id}")

        if project.owner_id != user_id:
            raise PermissionError("æ— æƒè®¿é—®è¯¥é¡¹ç›®")

        logger.info(f"å¼€å§‹æ„å»ºé¡¹ç›® {project_id} çš„æ–‡çŒ®ç´¢å¼•ï¼Œä»»åŠ¡ID: {task_id}")

        project.status = "indexing"
        project.updated_at = datetime.utcnow()
        db.commit()

        numeric_task_id: Optional[int] = None
        try:
            numeric_task_id = int(task_id)
        except (TypeError, ValueError):
            numeric_task_id = None

        task: Optional[Task] = None
        if numeric_task_id is not None:
            task = db.query(Task).filter(Task.id == numeric_task_id).first()

        if task:
            task_stream = TaskStreamService(db)

            async def run_indexing(progress_callback):
                literature_segments = db.query(LiteratureSegment).join(Literature).filter(
                    Literature.projects.any(id=project_id)
                ).all()

                await progress_callback(
                    "å‡†å¤‡æ„å»ºæ–‡çŒ®ç´¢å¼•",
                    5,
                    {"segment_count": len(literature_segments)},
                )

                return await _perform_indexing(
                    db,
                    project,
                    literature_segments,
                    progress_callback,
                )

            try:
                result = await task_stream.run_with_progress(
                    task,
                    "å‡†å¤‡æ„å»ºæ–‡çŒ®ç´¢å¼•",
                    run_indexing,
                )
                return result
            except Exception:
                project.status = "error"
                db.commit()
                raise

        from app.services.stream_progress_service import StreamProgressService

        progress_service = StreamProgressService()

        literature_segments = db.query(LiteratureSegment).join(Literature).filter(
            Literature.projects.any(id=project_id)
        ).all()

        estimated_time = (
            f"{len(literature_segments) * 2}-{len(literature_segments) * 5} ç§’"
            if literature_segments
            else "æ— æ³•ä¼°ç®—"
        )

        await progress_service.broadcast_project_update(
            project_id,
            {
                "type": "indexing_started",
                "task_id": task_id,
                "estimated_time": estimated_time,
            },
        )

        async def project_progress(step: str, progress: int, details: Dict[str, Any]):
            await progress_service.broadcast_project_update(
                project_id,
                {
                    "type": "indexing_progress",
                    "progress": progress,
                    "message": step,
                    **details,
                },
            )

        try:
            result = await _perform_indexing(
                db,
                project,
                literature_segments,
                project_progress,
            )

            await progress_service.broadcast_project_update(
                project_id,
                {
                    "type": "indexing_completed",
                    "segments_indexed": result.get("indexed_segments", 0),
                },
            )
            return result

        except Exception as exc:
            project.status = "error"
            db.commit()

            await progress_service.broadcast_project_update(
                project_id,
                {
                    "type": "indexing_failed",
                    "error": str(exc),
                },
            )
            logger.error(f"ç´¢å¼•æ„å»ºå¤±è´¥: {exc}")
            raise

    finally:
        db.close()


async def download_and_process_pdf_task(
    literature_id: int,
    user_id: int,
    task_id: Optional[int] = None,
):
    """ä»ResearchRabbitè‡ªåŠ¨ä¸‹è½½PDFå¹¶å¤„ç†çš„åå°ä»»åŠ¡ã€‚"""
    db = SessionLocal()
    task: Optional[Task] = None
    task_stream: Optional[TaskStreamService] = None
    token = None

    async def _run_pipeline(progress_callback: Callable[[str, int, Optional[Dict[str, Any]]], Awaitable[None]]):
        literature = (
            db.query(Literature)
            .filter(Literature.id == literature_id)
            .first()
        )
        if not literature:
            raise ValueError(f"æœªæ‰¾åˆ°æ–‡çŒ®è®°å½• ID: {literature_id}")

        project = (
            db.query(Project)
            .join(Project.literature)
            .filter(Literature.id == literature_id)
            .first()
        )
        project_id = project.id if project else None

        await progress_callback(
            "åˆå§‹åŒ–PDFå¤„ç†",
            5,
            {
                "literature_id": literature_id,
                "title": literature.title,
                "project_id": project_id,
            },
        )

        literature.parsing_status = "processing"
        db.commit()

        from app.services.research_rabbit_client import ResearchRabbitClient

        rabbit_client = ResearchRabbitClient()
        pdf_info = None
        if literature.doi:
            logger.info(f"é€šè¿‡DOIè·å–PDFä¿¡æ¯: {literature.doi}")
            await progress_callback(
                "æŸ¥è¯¢PDFä¿¡æ¯",
                15,
                {"doi": literature.doi},
            )
            pdf_info = await rabbit_client.get_pdf_info(literature.doi)

        if not pdf_info or not pdf_info.get("url_for_pdf"):
            logger.warning(f"æœªæ‰¾åˆ°PDFä¸‹è½½é“¾æ¥: {literature.title[:50]}")
            await create_basic_segments(literature, db, project)
            literature.parsing_status = "completed"
            db.commit()
            return {
                "status": "no_pdf",
                "message": f"æ–‡çŒ®å¤„ç†å®Œæˆï¼ˆæœªæ‰¾åˆ°PDFï¼‰: {literature.title[:50]}",
                "project_id": project_id,
                "segments_created": 0,
            }

        download_url = pdf_info["url_for_pdf"]
        logger.info(f"å¼€å§‹ä¸‹è½½PDF: {download_url}")
        await progress_callback(
            "ä¸‹è½½PDF",
            35,
            {"download_url": download_url},
        )
        pdf_data = await rabbit_client.download_pdf(download_url)

        if not pdf_data:
            logger.warning(f"PDFä¸‹è½½å¤±è´¥: {literature.title[:50]}")
            await create_basic_segments(literature, db, project)
            literature.parsing_status = "completed"
            db.commit()
            return {
                "status": "download_failed",
                "message": f"PDFä¸‹è½½å¤±è´¥: {literature.title[:50]}",
                "project_id": project_id,
                "segments_created": 0,
            }

        import tempfile
        import os

        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as temp_file:
            temp_file.write(pdf_data)
            temp_pdf_path = temp_file.name

        try:
            from app.services.pdf_processor import PDFProcessor

            await progress_callback(
                "è§£æPDFå†…å®¹",
                60,
                {"temp_path": temp_pdf_path},
            )

            pdf_processor = PDFProcessor()
            structure_template = project.structure_template if project and project.structure_template else None

            result = await pdf_processor.process_pdf_with_segments(
                temp_pdf_path,
                structure_template,
            )

            if result.get("success"):
                segments_data = result.get("segments", [])

                literature.parsing_status = "completed"
                literature.is_downloaded = True
                literature.is_parsed = True
                literature.pdf_path = temp_pdf_path

                for segment_data in segments_data:
                    segment = LiteratureSegment(
                        literature_id=literature.id,
                        segment_type=segment_data.get("segment_type", "paragraph"),
                        content=segment_data.get("content", ""),
                        order=segment_data.get("order", 0),
                        page_number=segment_data.get("page_number"),
                        metadata=segment_data.get("metadata", {}),
                    )
                    db.add(segment)

                db.commit()

                await progress_callback(
                    "PDFè§£æå®Œæˆ",
                    90,
                    {
                        "segments_created": len(segments_data),
                        "metadata": result.get("metadata", {}),
                    },
                )

                return {
                    "status": "success",
                    "message": f"PDFå¤„ç†æˆåŠŸ: {literature.title[:50]}",
                    "project_id": project_id,
                    "segments_created": len(segments_data),
                }

            logger.warning(f"PDFå¤„ç†å¤±è´¥: {result.get('error', 'Unknown error')}")
            await create_basic_segments(literature, db, project)
            literature.parsing_status = "completed"
            db.commit()

            return {
                "status": "processing_failed",
                "message": f"PDFå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬ä¿¡æ¯: {literature.title[:50]}",
                "project_id": project_id,
                "segments_created": 0,
            }

        finally:
            if os.path.exists(temp_pdf_path):
                try:
                    os.unlink(temp_pdf_path)
                except Exception as clean_error:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {clean_error}")

    try:
        if task_id is None:
            raise ValueError("PDFå¤„ç†ä»»åŠ¡å¿…é¡»ç»‘å®šæœ‰æ•ˆçš„ä»»åŠ¡ä¸Šä¸‹æ–‡")

        task = db.query(Task).filter(Task.id == task_id).first()
        if not task:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

        task_stream = TaskStreamService(db)
        token = task_cost_tracker.activate(task.id, db)
        return await task_stream.run_with_progress(
            task,
            "åˆå§‹åŒ–PDFå¤„ç†",
            _run_pipeline,
        )

    except Exception as exc:
        logger.error(f"PDFä¸‹è½½å¤„ç†ä»»åŠ¡å¤±è´¥ (æ–‡çŒ®ID: {literature_id}): {exc}")
        raise

    finally:
        if token is not None:
            task_cost_tracker.deactivate(token)
        db.close()


async def start_main_experience_generation_task(
    task_id: Optional[int],
    project_id: int,
    user_id: int,
):
    """ä¸»ç»éªŒç”Ÿæˆçš„åå°ä»»åŠ¡ã€‚"""
    db = SessionLocal()
    task: Optional[Task] = None
    task_stream: Optional[TaskStreamService] = None
    token = None

    async def _run_pipeline(progress_callback: Callable[[str, int, Optional[Dict[str, Any]]], Awaitable[None]]):
        # è·å–é¡¹ç›®ä¿¡æ¯
        project = db.query(Project).filter(Project.id == project_id).first()
        if not project:
            raise ValueError(f"æœªæ‰¾åˆ°é¡¹ç›® ID: {project_id}")

        await progress_callback(
            "å¼€å§‹ä¸»ç»éªŒç”Ÿæˆ",
            10,
            {
                "project_id": project_id,
                "project_name": project.title,
            },
        )

        # è·å–é¡¹ç›®çš„æ‰€æœ‰æ–‡çŒ®æ®µè½
        literature_segments = (
            db.query(LiteratureSegment)
            .join(Literature)
            .filter(Literature.project_id == project_id)
            .all()
        )

        if not literature_segments:
            raise ValueError(f"é¡¹ç›® {project_id} ä¸­æ²¡æœ‰æ–‡çŒ®æ®µè½ï¼Œæ— æ³•ç”Ÿæˆä¸»ç»éªŒ")

        await progress_callback(
            f"å‡†å¤‡å¤„ç† {len(literature_segments)} ä¸ªæ–‡çŒ®æ®µè½",
            20,
            {"total_segments": len(literature_segments)},
        )

        # åˆå§‹åŒ–ç»éªŒå¼•æ“
        experience_engine = EnhancedExperienceEngine()

        # åˆ›å»ºä¸»ç»éªŒ
        result = await experience_engine.create_main_experiences(
            project=project,
            literature_segments=literature_segments,
            progress_callback=progress_callback
        )

        await progress_callback(
            "ä¸»ç»éªŒç”Ÿæˆå®Œæˆ",
            100,
            {
                "main_experiences_created": result.get("main_experiences_count", 0),
                "project_id": project_id,
            },
        )

        return {
            "status": "success",
            "message": f"é¡¹ç›® {project.title} ä¸»ç»éªŒç”Ÿæˆå®Œæˆ",
            "project_id": project_id,
            "main_experiences": result,
        }

    try:
        # å¦‚æœæ²¡æœ‰æä¾›task_idï¼Œåˆ›å»ºæ–°ä»»åŠ¡
        if task_id is None:
            user = db.query(User).filter(User.id == user_id).first()
            if not user:
                raise ValueError(f"ç”¨æˆ·ä¸å­˜åœ¨: {user_id}")

            task = Task(
                type="main_experience_generation",
                description=f"ç”Ÿæˆé¡¹ç›® {project_id} çš„ä¸»ç»éªŒåº“",
                parameters={
                    "project_id": project_id,
                    "user_id": user_id,
                },
                user_id=user_id,
                status="running",
            )
            db.add(task)
            db.commit()
            task_id = task.id
        else:
            task = db.query(Task).filter(Task.id == task_id).first()
            if not task:
                raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

        task_stream = TaskStreamService(db)
        token = task_cost_tracker.activate(task.id, db)
        return await task_stream.run_with_progress(
            task,
            "åˆå§‹åŒ–ä¸»ç»éªŒç”Ÿæˆ",
            _run_pipeline,
        )

    except Exception as exc:
        logger.error(f"ä¸»ç»éªŒç”Ÿæˆä»»åŠ¡å¤±è´¥ (é¡¹ç›®ID: {project_id}): {exc}")
        raise

    finally:
        if token is not None:
            task_cost_tracker.deactivate(token)
        db.close()
